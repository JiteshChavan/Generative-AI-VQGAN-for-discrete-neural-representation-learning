{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch observations\n",
    "- for configure_optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75648"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "kernel_size = 3*3\n",
    "fan_in = 3 * kernel_size\n",
    "out_channels = 64\n",
    "out_channels2 = 128\n",
    "\n",
    "class Machine (nn.Module):    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.first = nn.Linear (fan_in, out_channels, bias=True)\n",
    "        self.second = nn.Conv2d (out_channels, out_channels2, 3, 1, 1)\n",
    "\n",
    "\n",
    "model = Machine ()\n",
    "num_paramters = sum (p.numel() for p in model.parameters())\n",
    "num_paramters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1792\n",
    "# outchannels, in_channels, kernelsize0, kernelsize1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramdict = {pn:p for pn,p in model.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paramdict) # weight bias weight bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 27])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramdict['first.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 64, 3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramdict['second.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramdict['second.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_var:0.0, a_std:0.0, a_mean:0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.9970), tensor(10.1498), tensor(103.0192))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros (768)\n",
    "b = torch.zeros(768)\n",
    "print (f\"a_var:{a.var()}, a_std:{a.std()}, a_mean:{a.mean()}\")\n",
    "for i in range(100):\n",
    "    a += (100)**-0.5 * torch.randn(768)\n",
    "    b += torch.randn(768)\n",
    "\n",
    "a.std(), b.std(), b.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 3\n",
    "out_channels = 64\n",
    "kernal_size = 3\n",
    "module = nn.Conv2d (in_channels, out_channels, kernal_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.weight.shape\n",
    "torch.nn.init._calculate_correct_fan(module.weight, mode='fan_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a[8:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a[8:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_shard = torch.ones (64, 3, 256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_shard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.weight:torch.Size([512, 512, 3, 3])\n",
      "model.0.bias:torch.Size([512])\n",
      "model.1.conv_projection.weight:torch.Size([512, 512, 3, 3])\n",
      "model.1.conv_projection.bias:torch.Size([512])\n",
      "model.1.block.0.group_norm.weight:torch.Size([512])\n",
      "model.1.block.0.group_norm.bias:torch.Size([512])\n",
      "model.1.block.2.weight:torch.Size([512, 512, 3, 3])\n",
      "model.1.block.2.bias:torch.Size([512])\n",
      "model.1.block.3.group_norm.weight:torch.Size([512])\n",
      "model.1.block.3.group_norm.bias:torch.Size([512])\n",
      "model.2.group_norm.group_norm.weight:torch.Size([512])\n",
      "model.2.group_norm.group_norm.bias:torch.Size([512])\n",
      "model.2.conv_attention.weight:torch.Size([1536, 512, 1, 1])\n",
      "model.2.conv_attention.bias:torch.Size([1536])\n",
      "model.2.conv_projection.weight:torch.Size([512, 512, 1, 1])\n",
      "model.2.conv_projection.bias:torch.Size([512])\n",
      "model.3.conv_projection.weight:torch.Size([512, 512, 3, 3])\n",
      "model.3.conv_projection.bias:torch.Size([512])\n",
      "model.3.block.0.group_norm.weight:torch.Size([512])\n",
      "model.3.block.0.group_norm.bias:torch.Size([512])\n",
      "model.3.block.2.weight:torch.Size([512, 512, 3, 3])\n",
      "model.3.block.2.bias:torch.Size([512])\n",
      "model.3.block.3.group_norm.weight:torch.Size([512])\n",
      "model.3.block.3.group_norm.bias:torch.Size([512])\n",
      "model.4.conv_projection.weight:torch.Size([512, 512, 3, 3])\n",
      "model.4.conv_projection.bias:torch.Size([512])\n",
      "model.4.block.0.group_norm.weight:torch.Size([512])\n",
      "model.4.block.0.group_norm.bias:torch.Size([512])\n",
      "model.4.block.2.weight:torch.Size([512, 512, 3, 3])\n",
      "model.4.block.2.bias:torch.Size([512])\n",
      "model.4.block.3.group_norm.weight:torch.Size([512])\n",
      "model.4.block.3.group_norm.bias:torch.Size([512])\n",
      "model.5.conv.weight:torch.Size([512, 512, 3, 3])\n",
      "model.5.conv.bias:torch.Size([512])\n",
      "model.6.conv_projection.weight:torch.Size([256, 256, 3, 3])\n",
      "model.6.conv_projection.bias:torch.Size([256])\n",
      "model.6.block.0.group_norm.weight:torch.Size([512])\n",
      "model.6.block.0.group_norm.bias:torch.Size([512])\n",
      "model.6.block.2.weight:torch.Size([256, 512, 3, 3])\n",
      "model.6.block.2.bias:torch.Size([256])\n",
      "model.6.block.3.group_norm.weight:torch.Size([256])\n",
      "model.6.block.3.group_norm.bias:torch.Size([256])\n",
      "model.6.channel_up.weight:torch.Size([256, 512, 1, 1])\n",
      "model.6.channel_up.bias:torch.Size([256])\n",
      "model.7.conv.weight:torch.Size([256, 256, 3, 3])\n",
      "model.7.conv.bias:torch.Size([256])\n",
      "model.8.conv_projection.weight:torch.Size([128, 128, 3, 3])\n",
      "model.8.conv_projection.bias:torch.Size([128])\n",
      "model.8.block.0.group_norm.weight:torch.Size([256])\n",
      "model.8.block.0.group_norm.bias:torch.Size([256])\n",
      "model.8.block.2.weight:torch.Size([128, 256, 3, 3])\n",
      "model.8.block.2.bias:torch.Size([128])\n",
      "model.8.block.3.group_norm.weight:torch.Size([128])\n",
      "model.8.block.3.group_norm.bias:torch.Size([128])\n",
      "model.8.channel_up.weight:torch.Size([128, 256, 1, 1])\n",
      "model.8.channel_up.bias:torch.Size([128])\n",
      "model.9.conv.weight:torch.Size([128, 128, 3, 3])\n",
      "model.9.conv.bias:torch.Size([128])\n",
      "model.10.conv_projection.weight:torch.Size([64, 64, 3, 3])\n",
      "model.10.conv_projection.bias:torch.Size([64])\n",
      "model.10.block.0.group_norm.weight:torch.Size([128])\n",
      "model.10.block.0.group_norm.bias:torch.Size([128])\n",
      "model.10.block.2.weight:torch.Size([64, 128, 3, 3])\n",
      "model.10.block.2.bias:torch.Size([64])\n",
      "model.10.block.3.group_norm.weight:torch.Size([64])\n",
      "model.10.block.3.group_norm.bias:torch.Size([64])\n",
      "model.10.channel_up.weight:torch.Size([64, 128, 1, 1])\n",
      "model.10.channel_up.bias:torch.Size([64])\n",
      "model.11.conv.weight:torch.Size([64, 64, 3, 3])\n",
      "model.11.conv.bias:torch.Size([64])\n",
      "model.12.group_norm.weight:torch.Size([64])\n",
      "model.12.group_norm.bias:torch.Size([64])\n",
      "model.14.weight:torch.Size([64, 64, 3, 3])\n",
      "model.14.bias:torch.Size([64])\n",
      "model.15.weight:torch.Size([3, 64, 3, 3])\n",
      "model.15.bias:torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from decoder import Decoder, DecoderConfig\n",
    "\n",
    "dec = Decoder (DecoderConfig)\n",
    "\n",
    "for pn,p in dec.named_parameters():\n",
    "    print (f\"{pn}:{p.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQGan(\n",
       "  (encoder): Encoder(\n",
       "    (initial_conv): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ResidualBlock(\n",
       "        (conv_projection): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (1): Swish()\n",
       "          (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (4): Swish()\n",
       "          (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (channel_up): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): DownSampleBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (conv_projection): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (1): Swish()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (4): Swish()\n",
       "          (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (4): DownSampleBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "      )\n",
       "      (5): ResidualBlock(\n",
       "        (conv_projection): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (1): Swish()\n",
       "          (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (4): Swish()\n",
       "          (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (channel_up): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (6): DownSampleBlock(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "      )\n",
       "      (7): ResidualBlock(\n",
       "        (conv_projection): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (1): Swish()\n",
       "          (2): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (4): Swish()\n",
       "          (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (channel_up): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (8): DownSampleBlock(\n",
       "        (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2))\n",
       "      )\n",
       "      (9): ResidualBlock(\n",
       "        (conv_projection): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (1): Swish()\n",
       "          (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (4): Swish()\n",
       "          (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (10): SelfAttention(\n",
       "        (group_norm): GroupNorm(\n",
       "          (group_norm): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "        )\n",
       "        (conv_attention): Conv2d(1024, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (conv_projection): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (11): ResidualBlock(\n",
       "        (conv_projection): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (1): Swish()\n",
       "          (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (4): Swish()\n",
       "          (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (12): GroupNorm(\n",
       "        (group_norm): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "      )\n",
       "      (13): Swish()\n",
       "      (14): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pre_quant_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (quantizer): Quantizer(\n",
       "    (codebook): Embedding(1024, 512)\n",
       "  )\n",
       "  (post_quant_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (decoder): Decoder(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ResidualBlock(\n",
       "        (conv_projection): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (1): Swish()\n",
       "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (4): Swish()\n",
       "          (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): SelfAttention(\n",
       "        (group_norm): GroupNorm(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        )\n",
       "        (conv_attention): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (conv_projection): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (conv_projection): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (1): Swish()\n",
       "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (4): Swish()\n",
       "          (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (conv_projection): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (1): Swish()\n",
       "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (4): Swish()\n",
       "          (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): UpSampleBlock(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (6): ResidualBlock(\n",
       "        (conv_projection): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (1): Swish()\n",
       "          (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (4): Swish()\n",
       "          (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (channel_up): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (7): UpSampleBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (8): ResidualBlock(\n",
       "        (conv_projection): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (1): Swish()\n",
       "          (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (4): Swish()\n",
       "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (channel_up): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (9): UpSampleBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (10): ResidualBlock(\n",
       "        (conv_projection): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (1): Swish()\n",
       "          (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(\n",
       "            (group_norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          )\n",
       "          (4): Swish()\n",
       "          (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (channel_up): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (11): UpSampleBlock(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (12): GroupNorm(\n",
       "        (group_norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "      )\n",
       "      (13): Swish()\n",
       "      (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqgan import VQGan\n",
    "from encoder import EncoderConfig\n",
    "from decoder import DecoderConfig\n",
    "from quantizer import QuantizerConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "gan = VQGan (EncoderConfig, QuantizerConfig, DecoderConfig)\n",
    "gan.eval()\n",
    "gan.to('cuda')\n",
    "with torch.no_grad():\n",
    "    x = torch.randn (4, 3, 256, 256).to('cuda')\n",
    "    y = gan.encoder(x)\n",
    "    loss, zq, encoding_indices = gan.quantizer(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0\n",
      "done 1\n",
      "done 2\n",
      "done 3\n",
      "done 4\n",
      "done 5\n",
      "done 6\n",
      "done 7\n",
      "done 8\n",
      "done 9\n",
      "done 10\n",
      "done 11\n",
      "done 12\n",
      "done 13\n",
      "done 14\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = zq\n",
    "    for i in range (len(gan.decoder.model)-1):\n",
    "        out = gan.decoder.model[i](out)\n",
    "        temp = out.mean()\n",
    "        #temp.backward()\n",
    "        print (\"done\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out.requires_grad_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = gan.decoder.model[-1](out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= final.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zen\\miniconda3\\envs\\tr\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Zen\\miniconda3\\envs\\tr\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "e:\\Research\\vincentVanGogh\\lpips.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from lpips import LPIPS\n",
    "vgg = LPIPS().eval()\n",
    "x = torch.randn (4,3,256,256)\n",
    "decoded_image = x\n",
    "reconstruction_loss = F.mse_loss(x, decoded_image, reduction='none')\n",
    "reconstruction_loss = reconstruction_loss.mean(dim=(1,2,3))\n",
    "\n",
    "perc_loss = vgg (x, decoded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 256, 256]), torch.Size([4]), torch.Size([4, 1, 1, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, reconstruction_loss.shape, perc_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantizer import Quantizer, QuantizerConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "quantizer = Quantizer (QuantizerConfig)\n",
    "\n",
    "ze =torch.randn (4, 512, 16, 16)\n",
    "qloss, quantized, indices= quantizer(ze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 16, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torch.randn (256, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512, 16, 16]), torch.Size([256, 512]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ze**2).shape, (vocab**2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 512])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ze = ze.permute(0, 2, 3, 1).contiguous()\n",
    "ze = ze.view (-1, 512)\n",
    "ze.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 1]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a -b 2 = a2 + b2 - 2 a b\n",
    "ze_squared = torch.sum (ze**2, dim=1, keepdim=True)\n",
    "vocab_squared = torch.sum (vocab**2, dim=1, keepdim=True)\n",
    "ze_squared.shape, vocab_squared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 256])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BHW C, C,VS\n",
    "ab2 = ze@vocab.transpose(0,1)\n",
    "ab2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 16, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim = 512\n",
    "vocab_size = 256\n",
    "embedding = torch.randn(vocab_size, latent_dim)\n",
    "z =torch.randn (4, latent_dim, 16, 16)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z.permute(0, 2, 3, 1).contiguous()\n",
    "z_flattened = z.view(-1, latent_dim)\n",
    "\n",
    "d = torch.sum(z_flattened**2, dim=1, keepdim=True) + \\\n",
    "torch.sum(embedding**2, dim=1) - \\\n",
    "2*(torch.matmul(z_flattened, embedding.t()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.sum(z_flattened**2, dim=1, keepdim=True)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.sum(embedding**2, dim=1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn (4,3,256,256)\n",
    "x2 = torch.randn (4,3,256,256)\n",
    "reconstruction_loss = torch.abs(x - x2)\n",
    "reconstruction_loss = reconstruction_loss.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zen\\miniconda3\\envs\\tr\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Zen\\miniconda3\\envs\\tr\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "e:\\Research\\vincentVanGogh\\lpips.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n"
     ]
    }
   ],
   "source": [
    "from lpips import LPIPS\n",
    "m = LPIPS().eval()\n",
    "x= x.to('cuda')\n",
    "x2= x2.to('cuda')\n",
    "m.to('cuda')\n",
    "\n",
    "prec = m(x,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc_rec = prec + reconstruction_loss\n",
    "perc_rec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
