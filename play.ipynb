{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trick's for Attention:\n",
    "- Permute (multi dimensional transpose into contiguous) is safer bet to manipulate matrices, or einsum works as awell\n",
    "- dont trust reshape or just view\n",
    "- dont try to do more than one thing at a time with view, be explicit\n",
    "- Keep note of Attn matrix (TxT specifies what your heads are looking at)\n",
    "- For convolutions: stride 1 keeps same size with padding = 1 if and only if image size is greater than kernel size\n",
    "- Don't expect coherent results trying to permute and view at the same time\n",
    "- expecting C2,h3,w3 to be formulated as all9pixels,2channels with .view(h3*w3, C2) is falliable\n",
    "\n",
    "## Tensor Mastery:\n",
    "- You can't view permuted tensor in a different shape later on directly, unless you specifically lay it out in a contiguous fashion\n",
    "- a.permute(blah blah) and then later on a=a.view(xy) doesnt work\n",
    "- do a.permute().cntiguous() then a=a.view() works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from modules import GroupNorm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[2., 2., 2.],\n",
       "          [2., 2., 2.],\n",
       "          [2., 2., 2.]],\n",
       " \n",
       "         [[2., 2., 2.],\n",
       "          [2., 2., 2.],\n",
       "          [2., 2., 2.]],\n",
       " \n",
       "         [[2., 2., 2.],\n",
       "          [2., 2., 2.],\n",
       "          [2., 2., 2.]],\n",
       " \n",
       "         [[2., 2., 2.],\n",
       "          [2., 2., 2.],\n",
       "          [2., 2., 2.]]]),)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = torch.ones(4, 3, 3, requires_grad=True)\n",
    "b = a.pow(2)\n",
    "loss = b.sum()\n",
    "z = torch.autograd.grad(loss, a, retain_graph=True)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor ([1, 2, 3, 4])\n",
    "b = torch.tensor ([100, 100, 100, 5 ])\n",
    "\n",
    "answer = torch.argmin (b-a, dim=0)\n",
    "answer.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor ([[3,1,1], [2,2,3]])\n",
    "a = a.view(2,3).unsqueeze(1)\n",
    "a[0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing broadcasting for quantization differences each BHW must be broadcasted VS times and have VS differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8651,  0.0284]],\n",
      "\n",
      "        [[ 0.5256, -0.3633]],\n",
      "\n",
      "        [[-0.4169, -1.2650]],\n",
      "\n",
      "        [[ 1.2367,  0.1980]]])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor([[-0.1349, -0.9716],\n",
      "        [-0.1349, -0.9716],\n",
      "        [-0.1349, -0.9716]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(32)\n",
    "B = 1\n",
    "H = 2\n",
    "W = 2\n",
    "BHW = 4\n",
    "vocab_size = 3\n",
    "C = 2\n",
    "X = torch.randn (BHW, C).unsqueeze (1) # (BHW, 1, C)\n",
    "emb = torch.randn (vocab_size, C)\n",
    "zeros = torch.ones_like(emb) # (VS, C)\n",
    "\n",
    "diff = X - zeros\n",
    "print (X)\n",
    "print (\"\\n\\n\\n\\n\\n\\n\")\n",
    "print (diff[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3]), torch.Size([4]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn (BHW, C).unsqueeze (1) # (BHW, 1, C)\n",
    "emb = torch.randn (vocab_size, C)\n",
    "zeros = torch.ones_like(emb)\n",
    "\n",
    "distances = (X - emb).pow(2).mean(dim=2) # BHW, VS (because mean along dim = 2)\n",
    "encoding_indices = torch.argmin(distances, dim=1)\n",
    "distances.shape, encoding_indices.shape # implies encoding indices lose the dim along which we find min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11, 12],\n",
       "         [11, 12],\n",
       "         [11, 12]],\n",
       "\n",
       "        [[21, 22],\n",
       "         [21, 22],\n",
       "         [21, 22]],\n",
       "\n",
       "        [[31, 32],\n",
       "         [31, 32],\n",
       "         [31, 32]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.tensor ([[[11,11,11],[12,12,12]], [[21,21,21],[22,22,22]], [[31,31,31],[32,32,32]]])\n",
    "test_permuted = test.permute (0, 2, 1).contiguous()\n",
    "test_permuted.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor ([[[11,11,11],[12,12,12]],[[21,21,21],[22,22,22]]])\n",
    "x=x.view(2*2*3).unsqueeze(1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn (4,16,16,1024)\n",
    "x = x.view (4*16*16, 1024)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 3, 3]), torch.Size([3]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = nn.Conv2d (3, 3, 3, stride=1, padding=1)\n",
    "test.weight.shape, test.bias.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1024, 16, 16]),\n",
       " torch.Size([4, 1024, 16, 16]),\n",
       " torch.Size([4, 3, 256, 256]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from modules import GroupNorm\n",
    "import math\n",
    "from quantizer import Quantizer, QuantizerConfig\n",
    "from encoder import Encoder, EncoderConfig\n",
    "from decoder import Decoder, DecoderConfig\n",
    "\n",
    "enc = Encoder (EncoderConfig)\n",
    "snap = Quantizer (QuantizerConfig)\n",
    "dec = Decoder (DecoderConfig)\n",
    "enc.to('cuda')\n",
    "snap.to('cuda')\n",
    "dec.to('cuda')\n",
    "X = torch.randn (4, 3, 256, 256)\n",
    "X = X.to ('cuda')\n",
    "ze = enc(X)\n",
    "vqloss, zq, indices = snap(ze)\n",
    "image = dec (zq)\n",
    "ze.shape, zq.shape, image.shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1024, 16, 16]),\n",
       " torch.Size([4, 1024, 16, 16]),\n",
       " torch.Size([4, 1024, 16, 16]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from encoder import Encoder\n",
    "from encoder import EncoderConfig\n",
    "test = Encoder (EncoderConfig)\n",
    "\n",
    "x = torch.randn (4, 3, 256,256)\n",
    "x = x.to ('cuda')\n",
    "test.to('cuda')\n",
    "#a0=test.latent_activation[0](x)\n",
    "#a1 =  test.latent_activation[1](a0)\n",
    "#a2 = test.latent_activation[2](a1)\n",
    "#a2.shape\n",
    "ze, mu, logvar = test (x)\n",
    "ze.shape, mu.shape, logvar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class isolatedSelfAttention (nn.Module):\n",
    "    def __init__ (self, channels):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        #norm\n",
    "        self.group_norm = GroupNorm (channels)\n",
    "        # attention\n",
    "        self.q = nn.Conv2d (channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.k = nn.Conv2d (channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.v = nn.Conv2d (channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        # projection\n",
    "        self.conv_projection = nn.Conv2d (channels, channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.y = None\n",
    "\n",
    "    def forward (self, X):\n",
    "        B, C, H, W = X.size ()\n",
    "\n",
    "        # normalize\n",
    "        X_normalized = self.group_norm (X) # (B, C, H, W)\n",
    "        # emit kqv\n",
    "        q = self.q (X_normalized)\n",
    "        k = self.k (X_normalized)\n",
    "        v = self.v (X_normalized) # (B, C, H, W)\n",
    "\n",
    "        k = k.view (B, C, H*W) # (B, C, HW)\n",
    "        q = q.view (B, C, H*W).transpose(1,2) # (B, HW, C)\n",
    "\n",
    "        attn= q @ k * (1.0/math.sqrt(C)) # (B, HW, HW)\n",
    "\n",
    "        attn = F.softmax (attn, dim=-1)\n",
    "\n",
    "        v = v.view (B, C, H*W).transpose(1,2)\n",
    "        self.y =  attn @ v # (B, HW, C)\n",
    "        self.y = self.y.transpose(1,2).contiguous().view (B, C, H, W)\n",
    "\n",
    "        self.y = self.conv_projection (self.y)\n",
    "\n",
    "        return X + self.y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1]],\n",
       "\n",
       "        [[2, 2, 2],\n",
       "         [2, 2, 2],\n",
       "         [2, 2, 2]],\n",
       "\n",
       "        [[3, 3, 3],\n",
       "         [3, 3, 3],\n",
       "         [3, 3, 3]],\n",
       "\n",
       "        [[4, 4, 4],\n",
       "         [4, 4, 4],\n",
       "         [4, 4, 4]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C H W\n",
    "a = torch.tensor ([\n",
    "                    [[1,1,1],[1,1,1],[1,1,1]], \n",
    "                    [[2,2,2],[2,2,2],[2,2,2]], \n",
    "                    [[3,3,3],[3,3,3],[3,3,3]],\n",
    "                    [[4,4,4,],[4,4,4,],[4,4,4,]]\n",
    "                ])\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4, 4, 4, 4, 4]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4, 3, 3\n",
    "# C, H, W -> nh, hs, H, W\n",
    "channels = 4\n",
    "n_head = 2\n",
    "h = 3\n",
    "w = 3\n",
    "b=a.view (n_head, channels//n_head, h*w)\n",
    "b[1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "         [4, 4, 4, 4, 4, 4, 4, 4, 4]]),\n",
       " tensor([1, 1, 1, 2, 2, 3, 3, 4, 4]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.view (n_head, channels//n_head, h*w)\n",
    "wrong = a.view (h*w, channels)\n",
    "c[1, :, :], wrong[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing einsum against permute into contiguous\n",
    "# Refer to modules.py for kernel fused F.scaled_dot_product_attention\n",
    "class SelfAttention (nn.Module):\n",
    "    def __init__(self, channels, n_head):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.n_head = n_head\n",
    "        assert channels % n_head == 0, f\"Specified channels:{channels} are not divisible by number of attention heads{n_head}\"\n",
    "        # norm\n",
    "        self.group_norm = GroupNorm (channels)\n",
    "        # attention\n",
    "        self.conv_attention = nn.Conv2d (channels, 3 * channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_projection = nn.Conv2d (channels, channels, kernel_size=1, stride=1, padding=0)\n",
    "    \n",
    "    def forward (self, X):\n",
    "        B, C, H, W = X.size()\n",
    "        # normalize X\n",
    "        x_normalized = self.group_norm (X)\n",
    "        # emit kqv\n",
    "        # X (B, C, H, W)\n",
    "        kqv = self.conv_attention (x_normalized) # (B, 3C, H, W)\n",
    "        q, k, v = kqv.split (self.channels ,dim=1) #(B,C,H,W) x3\n",
    "\n",
    "        # (B, C, H, W)\n",
    "        head_size = C//self.n_head\n",
    "\n",
    "        k=k.view (B, self.n_head, head_size, H*W).transpose(-1,-2) # (B, nh, HW, hs)\n",
    "        q = q.view (B, self.n_head, head_size, H*W) # (B, nh, hs, HW)\n",
    "        v=v.view (B, self.n_head, head_size, H*W) # (B, nh, hs, HW)\n",
    "\n",
    "        #(HW HW) @ (HW, hs)\n",
    "        att = k @ q * (1.0/math.sqrt(head_size))\n",
    "        att = F.softmax (att, dim=-1) # (B, nh, HW, HW)\n",
    "        y = att @ v.transpose (-1,-2) # (B, nh, HW, HW) @ (B, nh, HW, hs) -> (B, nh, HW, hs) same as B T T @ B T C\n",
    "        \n",
    "        y = y.permute (0, 1, 3, 2).contiguous ().view (B, C, H, W)\n",
    "        y = self.conv_projection(y)\n",
    "        return X + y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused vs Individual Attention | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed (32)\n",
    "X = torch.randn (2,64, 5,5)\n",
    "y = X.clone()\n",
    "_3xattn = SelfAttention (64)\n",
    "manual_attention = isolatedSelfAttention (64)\n",
    "\n",
    "_3xattn.conv_projection\n",
    "_3xattn_weights = (_3xattn.conv_attention.weight.data.clone())\n",
    "_3xattn_bias = (_3xattn.conv_attention.bias.data.clone())\n",
    "_3xattn_proj_weight = _3xattn.conv_projection.weight.data.clone()\n",
    "_3xattn_proj_bias = _3xattn.conv_projection.bias.data.clone()\n",
    "_3xattn_group_norm_weights = _3xattn.group_norm.group_norm.weight.data.clone()\n",
    "_3xattn_group_norm_biases = _3xattn.group_norm.group_norm.bias.data.clone()\n",
    "\n",
    "wq, wk, wv = _3xattn_weights.split (manual_attention.channels, dim=0)\n",
    "bq, bk, bv = _3xattn_bias.split (manual_attention.channels, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "manual_attention.q.weight.data = wq\n",
    "manual_attention.q.bias.data = bq\n",
    "manual_attention.k.weight.data = wk\n",
    "manual_attention.k.bias.data = bk\n",
    "manual_attention.v.weight.data = wv\n",
    "manual_attention.v.bias.data = bv\n",
    "manual_attention.conv_projection.weight.data = _3xattn_proj_weight\n",
    "manual_attention.conv_projection.bias.data = _3xattn_proj_bias\n",
    "manual_attention.group_norm.group_norm.weight.data = _3xattn_group_norm_weights\n",
    "manual_attention.group_norm.group_norm.bias.data = _3xattn_group_norm_biases\n",
    "\n",
    "out1 = _3xattn(X)\n",
    "out2 = manual_attention (X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)  # You can replace 42 with any integer of your choice\n",
    "\n",
    "# Set seed for CPU and GPU (if using CUDA)\n",
    "torch.manual_seed(42)  # For CPU\n",
    "\n",
    "# If using GPU (CUDA), set the seed for CUDA operations as well:\n",
    "torch.cuda.manual_seed(42)  # For the current GPU device\n",
    "torch.cuda.manual_seed_all(42)  # For all GPUs if using multiple GPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class for Loading Images\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "        # Get all the jpeg files from the folder\n",
    "        \n",
    "        self.image_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.jpeg') or f.lower().endswith('.jpg')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.folder_path, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('RGB')  # Convert to RGB if it's not already\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations (resizing, conversion to tensor, normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.ToTensor(),          # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = ImageDataset('./crushit2', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = DataLoader(dataset, batch_size=800, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Shape of the batch: torch.Size([800, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(batches):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(f\"Shape of the batch: {batch.shape}\")  # (batch_size, channels, height, width)\n",
    "    # print(len(batch))\n",
    "    \n",
    "    # You can now pass this batch to your CNN\n",
    "    # For example, to pass to a model:\n",
    "    # output = model(images)\n",
    "    \n",
    "    # Break after one batch for example purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 3, 256, 256])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 256, 256, 3])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr = Xtr.permute (0, 2,3, 1)\n",
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work group norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 256, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Input tensor (batch of RGB images)\n",
    "B, C_in, H, W = 4, 3, 256, 256  # Input: RGB images\n",
    "images = torch.randn(B, C_in, H, W)\n",
    "# Step 2: Convolution to increase the number of channels\n",
    "conv = nn.Conv2d(in_channels=C_in, out_channels=256, kernel_size=3, stride=1, padding = 1)\n",
    "features = conv(images)  # Output: B x 256 x 256 x 256\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNorm2D (nn.Module):\n",
    "    def __init__ (self, num_groups, num_channels, num_spatial_channels, affine=True, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.num_groups = num_groups\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        assert self.num_channels % self.num_groups == 0, f\"channels :{num_channels} are not divisible by {num_groups} groups\"\n",
    "\n",
    "        # Learnable parameters\n",
    "        if affine == True:\n",
    "            # scale\n",
    "            self.gamma = nn.Parameter(torch.ones(1, num_channels, *[1] * num_spatial_channels))\n",
    "            # shift\n",
    "            self.beta = nn.Parameter (torch.zeros (1, num_channels, *[1] * num_spatial_channels))\n",
    "\n",
    "    def forward (self, X):\n",
    "        B, C = X.shape[:2]\n",
    "        spatial_dims = X.shape[2:]\n",
    "        assert C == self.num_channels, f\"Mismatch between input channels: {C} and num_channels: {self.num_channels} at initialization\"\n",
    "\n",
    "        # rearrage the input in shape of groups as an extra batch dimension\n",
    "        G = self.num_groups\n",
    "        group_size = C // G\n",
    "        X = X.view (B, G, group_size, *spatial_dims)\n",
    "\n",
    "        print (f\"Now shape of X after arranging in groups is {X.shape}\")\n",
    "        # compute mean and variance across group and spatial dimensions\n",
    "        dims_to_reduce = tuple (range (2, X.dim())) # All Dimensions except B and G\n",
    "        X_mean_no_keep_dim = X.mean (dim=dims_to_reduce, keepdim=False)\n",
    "        X_var_no_keep_dim = X.var (dim= dims_to_reduce, keepdim=False)\n",
    "        X_mean = X.mean (dim=dims_to_reduce, keepdim=True)\n",
    "        X_var = X.var (dim= dims_to_reduce, keepdim=True)\n",
    "        print (f\"Shapes of means: NoKeepDim: {X_mean_no_keep_dim.shape} KeepDim: {X_mean.shape}\")\n",
    "        print (f\"Shapes of vars: NoKeepDim: {X_var_no_keep_dim.shape} KeepDim: {X_var.shape}\")\n",
    "\n",
    "        # normalize:\n",
    "        X = (X - X_mean) / torch.sqrt(X_var+self.eps)\n",
    "        print (f\"Intermediate shape just after normalization = {X.shape}\")\n",
    "        # reshape back to original shape\n",
    "        X = X.view (B, C, *spatial_dims)\n",
    "        print (f\"Final shape after norm = {X.shape}\")\n",
    "        # Scale and shift\n",
    "        return self.gamma * X + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual layers to pytorch layers\n",
    "def cmp(s, custom, torch_version):\n",
    "    ex = torch.all(custom == torch_version)\n",
    "    app = torch.allclose(custom, torch_version, rtol=1e-5, atol=1e-7)\n",
    "    maxdiff = (custom - torch_version).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex.item()):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's work Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Initialize conv layer parameters\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels, *kernel_size) * \n",
    "            (1.0 / (in_channels * kernel_size[0] * kernel_size[1]))**0.5\n",
    "        )\n",
    "        self.bias = nn.Parameter(torch.randn(out_channels)) if bias else None\n",
    "\n",
    "    def forward(self, X):\n",
    "        B, Cin, H, W = X.shape\n",
    "        assert Cin == self.in_channels, (\n",
    "            f\"In channels of input tensor: {Cin} don't match the conv layer initialization channels: {self.in_channels}\"\n",
    "        )\n",
    "\n",
    "        # pad the input (B, Cin, H, W) W->p[1] H->p[0]\n",
    "        X = F.pad(X, (self.padding[1], self.padding[1], self.padding[0], self.padding[0]))\n",
    "\n",
    "        # Compute Output Dimensions\n",
    "        H_out = ((2 * self.padding[0] + H) - self.kernel_size[0]) // self.stride[0] + 1\n",
    "        W_out = ((2 * self.padding[1] + W) - self.kernel_size[1]) // self.stride[1] + 1\n",
    "\n",
    "        # Extract the striding blocks from input\n",
    "        # (4, 3, 9, 9) -> with kernel size 3 and stride 1 yields (4, 9*3, 7*7) = (4, 27, 49)\n",
    "        # in general unfold on (B, Cin, H, W) yields (B, total spots in one stride block, number of total striding patch blocks in the image)\n",
    "        X = F.unfold(X, kernel_size=self.kernel_size, stride=self.stride)  # (B, Cin * kh * kw, L)\n",
    "\n",
    "        #re-arrange weights\n",
    "        # suppose out channels = 6 kernel size 3x3\n",
    "        # weights initialized as (6, 3, 3, 3)\n",
    "        # arrange them as (6, 27) because we have flattened our conv patch blocks as well\n",
    "        weight_flat = self.weight.view(self.out_channels, -1)  # (out_channels, kernel_size[0]*kernel_size[1]*in_channels) i.e. (6, 27)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        # X: (B, Cin * kh * kw, L), weight_flat: (out_channels, Cin * kh * kw)\n",
    "\n",
    "        # BE CAREFUL! ABOUT mutating views like (B, 49, 6) as (B, 6, 7, 7)\n",
    "        # rows and columns might get wrongly permuted\n",
    "        # SAFE TODO: (B, 6, 49) -> (B, 6, 7, 7)\n",
    "        out = torch.einsum('bkl, ok -> bol', X, weight_flat)  # (B, out_channels, L)\n",
    "\n",
    "        # Add bias if available\n",
    "        if self.bias is not None:\n",
    "            out += self.bias.view(1, self.out_channels, 1)  # Broadcasting bias\n",
    "\n",
    "        # Reshape to output dimensions\n",
    "        out = out.view(B, self.out_channels, H_out, W_out)  # (B, out_channels, H_out, W_out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference: 2.384185791015625e-06\n"
     ]
    }
   ],
   "source": [
    "B, Cin, H, W = 1, 4, 400, 400\n",
    "out_channels, kernel_size = 3, (4, 4)\n",
    "stride, padding = (1, 1), (0, 0)\n",
    "\n",
    "torch.manual_seed(32)\n",
    "input = torch.randn(B, Cin, H, W)\n",
    "\n",
    "convLayer = Conv2D(Cin, out_channels, kernel_size, stride=stride, padding=padding, bias=True)\n",
    "myOut = convLayer(input)\n",
    "\n",
    "torchConv = nn.Conv2d(Cin, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
    "torchConv.weight = nn.Parameter(convLayer.weight.data.clone())\n",
    "torchConv.bias = nn.Parameter(convLayer.bias.data.clone())\n",
    "torchOut = torchConv(input)\n",
    "\n",
    "# Compare outputs\n",
    "print(\"Max difference:\", (myOut - torchOut).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now shape of X after arranging in groups is torch.Size([4, 32, 8, 20, 20, 20, 20])\n",
      "Shapes of means: NoKeepDim: torch.Size([4, 32]) KeepDim: torch.Size([4, 32, 1, 1, 1, 1, 1])\n",
      "Shapes of vars: NoKeepDim: torch.Size([4, 32]) KeepDim: torch.Size([4, 32, 1, 1, 1, 1, 1])\n",
      "Intermediate shape just after normalization = torch.Size([4, 32, 8, 20, 20, 20, 20])\n",
      "Final shape after norm = torch.Size([4, 256, 20, 20, 20, 20])\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Input tensor (batch of RGB images)\n",
    "B, C_in, H, W, Z, Y = 4, 256, 20, 20, 20, 20# Input: RGB images\n",
    "images = torch.randn(B, C_in, H, W, Z, Y)\n",
    "\n",
    "normModule = GroupNorm2D (32, 256, 4, affine=True)\n",
    "torch_group_norm = nn.GroupNorm(num_groups=32, num_channels=256, affine=True)\n",
    "\n",
    "answer = normModule (images)\n",
    "torch_answer = torch_group_norm (images)\n",
    "\n",
    "B, Cin, H, W = 1, 4, 400, 400\n",
    "out_channels, kernel_size = 3, (4, 4)\n",
    "stride, padding = (1, 1), (0, 0)\n",
    "\n",
    "torch.manual_seed(32)\n",
    "input = torch.randn(B, Cin, H, W)\n",
    "\n",
    "convLayer = Conv2D(Cin, out_channels, kernel_size, stride=stride, padding=padding, bias=True)\n",
    "myConv2D = convLayer(input)\n",
    "\n",
    "torchConv = nn.Conv2d(Cin, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
    "torchConv.weight = nn.Parameter(convLayer.weight.data.clone())\n",
    "torchConv.bias = nn.Parameter(convLayer.bias.data.clone())\n",
    "torchConv2D = torchConv(input)\n",
    "\n",
    "\n",
    "print (\"\\n\\n\")\n",
    "# TODO: Documentation; change tolerances later for approx,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's work UpSample and DownSample blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSample (nn.Module):\n",
    "    def __init__(self, channels, factor=2):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "        self.conv = nn.Conv2d (channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward (self, X):\n",
    "        X = F.interpolate (X, scale_factor=self.factor)\n",
    "        X = self.conv (X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "aold = torch.randn (1,2, 2,2)\n",
    "upsampler = UpSample (2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-2.3440, -0.3994],\n",
       "           [-1.2656, -1.4913]],\n",
       " \n",
       "          [[-0.4680,  1.5279],\n",
       "           [-0.4927,  0.1401]]]]),\n",
       " tensor([[[[-2.3440, -2.3440, -0.3994, -0.3994],\n",
       "           [-2.3440, -2.3440, -0.3994, -0.3994],\n",
       "           [-1.2656, -1.2656, -1.4913, -1.4913],\n",
       "           [-1.2656, -1.2656, -1.4913, -1.4913]],\n",
       " \n",
       "          [[-0.4680, -0.4680,  1.5279,  1.5279],\n",
       "           [-0.4680, -0.4680,  1.5279,  1.5279],\n",
       "           [-0.4927, -0.4927,  0.1401,  0.1401],\n",
       "           [-0.4927, -0.4927,  0.1401,  0.1401]]]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampleda = F.interpolate (aold, scale_factor=2.0)\n",
    "aold, upsampleda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3], [2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= F.pad (x, (0,1,0,1), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 0],\n",
       "        [2, 3, 4, 0],\n",
       "        [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(\n",
    "    [[[1, 1, 1],[1, 1, 1],[1, 1, 1]], [[2, 2, 2],[2, 2, 2],[2, 2, 2]]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1]],\n",
       "\n",
       "        [[1, 1],\n",
       "         [1, 2],\n",
       "         [2, 2]],\n",
       "\n",
       "        [[2, 2],\n",
       "         [2, 2],\n",
       "         [2, 2]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view (3,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 2],\n",
       "          [1, 2],\n",
       "          [1, 2]],\n",
       " \n",
       "         [[1, 2],\n",
       "          [1, 2],\n",
       "          [1, 2]],\n",
       " \n",
       "         [[1, 2],\n",
       "          [1, 2],\n",
       "          [1, 2]]]),\n",
       " torch.Size([3, 3, 2]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.permute (1,2,0)\n",
    "a, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(\n",
    "    [[[1, 1, 1],[1, 1, 1],[1, 1, 1]], [[2, 2, 2],[2, 2, 2],[2, 2, 2]]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1]],\n",
       "\n",
       "        [[2, 2, 2],\n",
       "         [2, 2, 2],\n",
       "         [2, 2, 2]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 1],\n",
       "          [1, 1],\n",
       "          [1, 1]],\n",
       " \n",
       "         [[1, 1],\n",
       "          [1, 2],\n",
       "          [2, 2]],\n",
       " \n",
       "         [[2, 2],\n",
       "          [2, 2],\n",
       "          [2, 2]]]),\n",
       " tensor([[[1, 2],\n",
       "          [1, 2],\n",
       "          [1, 2]],\n",
       " \n",
       "         [[1, 2],\n",
       "          [1, 2],\n",
       "          [1, 2]],\n",
       " \n",
       "         [[1, 2],\n",
       "          [1, 2],\n",
       "          [1, 2]]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.reshape (3,3,2)\n",
    "a = a.permute (1,2,0)\n",
    "b, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparisions between implementations from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_norm      | exact: False | approximate: True  | maxdiff: 2.86102294921875e-06\n",
      "conv2d          | exact: False | approximate: False | maxdiff: 2.384185791015625e-06\n",
      "Fused vs Individual Attention | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp (\"group_norm\", answer, torch_answer)\n",
    "cmp (\"conv2d\", myConv2D, torchConv2D)\n",
    "cmp ('Fused vs Individual Attention',out1, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
