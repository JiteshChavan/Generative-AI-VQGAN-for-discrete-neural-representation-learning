{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)  # You can replace 42 with any integer of your choice\n",
    "\n",
    "# Set seed for CPU and GPU (if using CUDA)\n",
    "torch.manual_seed(42)  # For CPU\n",
    "\n",
    "# If using GPU (CUDA), set the seed for CUDA operations as well:\n",
    "torch.cuda.manual_seed(42)  # For the current GPU device\n",
    "torch.cuda.manual_seed_all(42)  # For all GPUs if using multiple GPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class for Loading Images\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "        # Get all the jpeg files from the folder\n",
    "        \n",
    "        self.image_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.jpeg') or f.lower().endswith('.jpg')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.folder_path, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('RGB')  # Convert to RGB if it's not already\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations (resizing, conversion to tensor, normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.ToTensor(),          # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = ImageDataset('./crushit2', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = DataLoader(dataset, batch_size=800, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Shape of the batch: torch.Size([800, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(batches):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(f\"Shape of the batch: {batch.shape}\")  # (batch_size, channels, height, width)\n",
    "    # print(len(batch))\n",
    "    \n",
    "    # You can now pass this batch to your CNN\n",
    "    # For example, to pass to a model:\n",
    "    # output = model(images)\n",
    "    \n",
    "    # Break after one batch for example purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 3, 256, 256])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 256, 256, 3])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr = Xtr.permute (0, 2,3, 1)\n",
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvLayer (nn.Module):\n",
    "    # in_channels RGB 3, out_channels\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride_length, padding=0):\n",
    "        super ().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride_length = stride_length\n",
    "        self.padding = padding\n",
    "\n",
    "        # initialize the weights and biases for kernels\n",
    "        self.weights = \n",
    "\n",
    "class CNNEncoder (nn.Module):\n",
    "    def __init__ (self, ):\n",
    "        # list of layers (kernel, stride)\n",
    "        # \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work group norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 256, 256])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Input tensor (batch of RGB images)\n",
    "B, C_in, H, W = 4, 3, 256, 256  # Input: RGB images\n",
    "images = torch.randn(B, C_in, H, W)\n",
    "# Step 2: Convolution to increase the number of channels\n",
    "conv = nn.Conv2d(in_channels=C_in, out_channels=256, kernel_size=3, stride=1, padding = 1)\n",
    "features = conv(images)  # Output: B x 256 x 256 x 256\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNorm2D (nn.Module):\n",
    "    def __init__ (self, num_groups, num_channels, num_spatial_channels, affine=True, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.num_groups = num_groups\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        assert self.num_channels % self.num_groups == 0, f\"channels :{num_channels} are not divisible by {num_groups} groups\"\n",
    "\n",
    "        # Learnable parameters\n",
    "        if affine == True:\n",
    "            # scale\n",
    "            self.gamma = nn.Parameter(torch.ones(1, num_channels, *[1] * num_spatial_channels))\n",
    "            # shift\n",
    "            self.beta = nn.Parameter (torch.zeros (1, num_channels, *[1] * num_spatial_channels))\n",
    "\n",
    "    def forward (self, X):\n",
    "        B, C = X.shape[:2]\n",
    "        spatial_dims = X.shape[2:]\n",
    "        assert C == self.num_channels, f\"Mismatch between input channels: {C} and num_channels: {self.num_channels} at initialization\"\n",
    "\n",
    "        # rearrage the input in shape of groups as an extra batch dimension\n",
    "        G = self.num_groups\n",
    "        group_size = C // G\n",
    "        X = X.view (B, G, group_size, *spatial_dims)\n",
    "\n",
    "        print (f\"Now shape of X after arranging in groups is {X.shape}\")\n",
    "        # compute mean and variance across group and spatial dimensions\n",
    "        dims_to_reduce = tuple (range (2, X.dim())) # All Dimensions except B and G\n",
    "        X_mean_no_keep_dim = X.mean (dim=dims_to_reduce, keepdim=False)\n",
    "        X_var_no_keep_dim = X.var (dim= dims_to_reduce, keepdim=False)\n",
    "        X_mean = X.mean (dim=dims_to_reduce, keepdim=True)\n",
    "        X_var = X.var (dim= dims_to_reduce, keepdim=True)\n",
    "        print (f\"Shapes of means: NoKeepDim: {X_mean_no_keep_dim.shape} KeepDim: {X_mean.shape}\")\n",
    "        print (f\"Shapes of vars: NoKeepDim: {X_var_no_keep_dim.shape} KeepDim: {X_var.shape}\")\n",
    "\n",
    "        # normalize:\n",
    "        X = (X - X_mean) / torch.sqrt(X_var+self.eps)\n",
    "        print (f\"Intermediate shape just after normalization = {X.shape}\")\n",
    "        # reshape back to original shape\n",
    "        X = X.view (B, C, *spatial_dims)\n",
    "        print (f\"Final shape after norm = {X.shape}\")\n",
    "        # Scale and shift\n",
    "        return self.gamma * X + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, custom, torch_version):\n",
    "    ex = torch.all(custom == torch_version)\n",
    "    app = torch.allclose(custom, torch_version, rtol=1e-5, atol=1e-7)\n",
    "    maxdiff = (custom - torch_version).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex.item()):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now shape of X after arranging in groups is torch.Size([4, 32, 8, 20, 20, 20, 20])\n",
      "Shapes of means: NoKeepDim: torch.Size([4, 32]) KeepDim: torch.Size([4, 32, 1, 1, 1, 1, 1])\n",
      "Shapes of vars: NoKeepDim: torch.Size([4, 32]) KeepDim: torch.Size([4, 32, 1, 1, 1, 1, 1])\n",
      "Intermediate shape just after normalization = torch.Size([4, 32, 8, 20, 20, 20, 20])\n",
      "Final shape after norm = torch.Size([4, 256, 20, 20, 20, 20])\n",
      "group_norm      | exact: False | approximate: True  | maxdiff: 2.86102294921875e-06\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Input tensor (batch of RGB images)\n",
    "B, C_in, H, W, Z, Y = 4, 256, 20, 20, 20, 20# Input: RGB images\n",
    "images = torch.randn(B, C_in, H, W, Z, Y)\n",
    "\n",
    "normModule = GroupNorm2D (32, 256, 4, affine=True)\n",
    "torch_group_norm = nn.GroupNorm(num_groups=32, num_channels=256, affine=True)\n",
    "\n",
    "answer = normModule (images)\n",
    "torch_answer = torch_group_norm (images)\n",
    "\n",
    "cmp (\"group_norm\", answer, torch_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.randn (1,2,3,4,5)\n",
    "0 , 1, 2, 3, 4\n",
    "X.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1]]* 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [1], [1], [1], [1]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tr2)",
   "language": "python",
   "name": "tr2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
