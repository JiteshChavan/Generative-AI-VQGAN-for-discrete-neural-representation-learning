{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)  # You can replace 42 with any integer of your choice\n",
    "\n",
    "# Set seed for CPU and GPU (if using CUDA)\n",
    "torch.manual_seed(42)  # For CPU\n",
    "\n",
    "# If using GPU (CUDA), set the seed for CUDA operations as well:\n",
    "torch.cuda.manual_seed(42)  # For the current GPU device\n",
    "torch.cuda.manual_seed_all(42)  # For all GPUs if using multiple GPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class for Loading Images\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "        # Get all the jpeg files from the folder\n",
    "        \n",
    "        self.image_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.jpeg') or f.lower().endswith('.jpg')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.folder_path, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('RGB')  # Convert to RGB if it's not already\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations (resizing, conversion to tensor, normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.ToTensor(),          # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = ImageDataset('./crushit2', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = DataLoader(dataset, batch_size=800, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Shape of the batch: torch.Size([800, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(batches):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(f\"Shape of the batch: {batch.shape}\")  # (batch_size, channels, height, width)\n",
    "    # print(len(batch))\n",
    "    \n",
    "    # You can now pass this batch to your CNN\n",
    "    # For example, to pass to a model:\n",
    "    # output = model(images)\n",
    "    \n",
    "    # Break after one batch for example purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 3, 256, 256])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 256, 256, 3])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr = Xtr.permute (0, 2,3, 1)\n",
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work group norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 256, 256])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Input tensor (batch of RGB images)\n",
    "B, C_in, H, W = 4, 3, 256, 256  # Input: RGB images\n",
    "images = torch.randn(B, C_in, H, W)\n",
    "# Step 2: Convolution to increase the number of channels\n",
    "conv = nn.Conv2d(in_channels=C_in, out_channels=256, kernel_size=3, stride=1, padding = 1)\n",
    "features = conv(images)  # Output: B x 256 x 256 x 256\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNorm2D (nn.Module):\n",
    "    def __init__ (self, num_groups, num_channels, num_spatial_channels, affine=True, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.num_groups = num_groups\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        assert self.num_channels % self.num_groups == 0, f\"channels :{num_channels} are not divisible by {num_groups} groups\"\n",
    "\n",
    "        # Learnable parameters\n",
    "        if affine == True:\n",
    "            # scale\n",
    "            self.gamma = nn.Parameter(torch.ones(1, num_channels, *[1] * num_spatial_channels))\n",
    "            # shift\n",
    "            self.beta = nn.Parameter (torch.zeros (1, num_channels, *[1] * num_spatial_channels))\n",
    "\n",
    "    def forward (self, X):\n",
    "        B, C = X.shape[:2]\n",
    "        spatial_dims = X.shape[2:]\n",
    "        assert C == self.num_channels, f\"Mismatch between input channels: {C} and num_channels: {self.num_channels} at initialization\"\n",
    "\n",
    "        # rearrage the input in shape of groups as an extra batch dimension\n",
    "        G = self.num_groups\n",
    "        group_size = C // G\n",
    "        X = X.view (B, G, group_size, *spatial_dims)\n",
    "\n",
    "        print (f\"Now shape of X after arranging in groups is {X.shape}\")\n",
    "        # compute mean and variance across group and spatial dimensions\n",
    "        dims_to_reduce = tuple (range (2, X.dim())) # All Dimensions except B and G\n",
    "        X_mean_no_keep_dim = X.mean (dim=dims_to_reduce, keepdim=False)\n",
    "        X_var_no_keep_dim = X.var (dim= dims_to_reduce, keepdim=False)\n",
    "        X_mean = X.mean (dim=dims_to_reduce, keepdim=True)\n",
    "        X_var = X.var (dim= dims_to_reduce, keepdim=True)\n",
    "        print (f\"Shapes of means: NoKeepDim: {X_mean_no_keep_dim.shape} KeepDim: {X_mean.shape}\")\n",
    "        print (f\"Shapes of vars: NoKeepDim: {X_var_no_keep_dim.shape} KeepDim: {X_var.shape}\")\n",
    "\n",
    "        # normalize:\n",
    "        X = (X - X_mean) / torch.sqrt(X_var+self.eps)\n",
    "        print (f\"Intermediate shape just after normalization = {X.shape}\")\n",
    "        # reshape back to original shape\n",
    "        X = X.view (B, C, *spatial_dims)\n",
    "        print (f\"Final shape after norm = {X.shape}\")\n",
    "        # Scale and shift\n",
    "        return self.gamma * X + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual layers to pytorch layers\n",
    "def cmp(s, custom, torch_version):\n",
    "    ex = torch.all(custom == torch_version)\n",
    "    app = torch.allclose(custom, torch_version, rtol=1e-5, atol=1e-7)\n",
    "    maxdiff = (custom - torch_version).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex.item()):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now shape of X after arranging in groups is torch.Size([4, 32, 8, 20, 20, 20, 20])\n",
      "Shapes of means: NoKeepDim: torch.Size([4, 32]) KeepDim: torch.Size([4, 32, 1, 1, 1, 1, 1])\n",
      "Shapes of vars: NoKeepDim: torch.Size([4, 32]) KeepDim: torch.Size([4, 32, 1, 1, 1, 1, 1])\n",
      "Intermediate shape just after normalization = torch.Size([4, 32, 8, 20, 20, 20, 20])\n",
      "Final shape after norm = torch.Size([4, 256, 20, 20, 20, 20])\n",
      "group_norm      | exact: False | approximate: True  | maxdiff: 2.86102294921875e-06\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's work Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Initialize conv layer parameters\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels, *kernel_size) * \n",
    "            (1.0 / (in_channels * kernel_size[0] * kernel_size[1]))**0.5\n",
    "        )\n",
    "        self.bias = nn.Parameter(torch.randn(out_channels)) if bias else None\n",
    "\n",
    "    def forward(self, X):\n",
    "        B, Cin, H, W = X.shape\n",
    "        assert Cin == self.in_channels, (\n",
    "            f\"In channels of input tensor: {Cin} don't match the conv layer initialization channels: {self.in_channels}\"\n",
    "        )\n",
    "\n",
    "        # pad the input (B, Cin, H, W) W->p[1] H->p[0]\n",
    "        X = F.pad(X, (self.padding[1], self.padding[1], self.padding[0], self.padding[0]))\n",
    "\n",
    "        # Compute Output Dimensions\n",
    "        H_out = ((2 * self.padding[0] + H) - self.kernel_size[0]) // self.stride[0] + 1\n",
    "        W_out = ((2 * self.padding[1] + W) - self.kernel_size[1]) // self.stride[1] + 1\n",
    "\n",
    "        # Extract the striding blocks from input\n",
    "        # (4, 3, 9, 9) -> with kernel size 3 and stride 1 yields (4, 9*3, 7*7) = (4, 27, 49)\n",
    "        # in general unfold on (B, Cin, H, W) yields (B, total spots in one stride block, number of total striding patch blocks in the image)\n",
    "        X = F.unfold(X, kernel_size=self.kernel_size, stride=self.stride)  # (B, Cin * kh * kw, L)\n",
    "\n",
    "        #re-arrange weights\n",
    "        # suppose out channels = 6 kernel size 3x3\n",
    "        # weights initialized as (6, 3, 3, 3)\n",
    "        # arrange them as (6, 27) because we have flattened our conv patch blocks as well\n",
    "        weight_flat = self.weight.view(self.out_channels, -1)  # (out_channels, kernel_size[0]*kernel_size[1]*in_channels) i.e. (6, 27)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        # X: (B, Cin * kh * kw, L), weight_flat: (out_channels, Cin * kh * kw)\n",
    "\n",
    "        # BE CAREFUL! ABOUT mutating views like (B, 49, 6) as (B, 6, 7, 7)\n",
    "        # rows and columns might get wrongly permuted\n",
    "        # SAFE TODO: (B, 6, 49) -> (B, 6, 7, 7)\n",
    "        out = torch.einsum('bkl, ok -> bol', X, weight_flat)  # (B, out_channels, L)\n",
    "\n",
    "        # Add bias if available\n",
    "        if self.bias is not None:\n",
    "            out += self.bias.view(1, self.out_channels, 1)  # Broadcasting bias\n",
    "\n",
    "        # Reshape to output dimensions\n",
    "        out = out.view(B, self.out_channels, H_out, W_out)  # (B, out_channels, H_out, W_out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference: 2.384185791015625e-06\n"
     ]
    }
   ],
   "source": [
    "B, Cin, H, W = 1, 4, 400, 400\n",
    "out_channels, kernel_size = 3, (4, 4)\n",
    "stride, padding = (1, 1), (0, 0)\n",
    "\n",
    "torch.manual_seed(32)\n",
    "input = torch.randn(B, Cin, H, W)\n",
    "\n",
    "convLayer = Conv2D(Cin, out_channels, kernel_size, stride=stride, padding=padding, bias=True)\n",
    "myOut = convLayer(input)\n",
    "\n",
    "torchConv = nn.Conv2d(Cin, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
    "torchConv.weight = nn.Parameter(convLayer.weight.data.clone())\n",
    "torchConv.bias = nn.Parameter(convLayer.bias.data.clone())\n",
    "torchOut = torchConv(input)\n",
    "\n",
    "# Compare outputs\n",
    "print(\"Max difference:\", (myOut - torchOut).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now shape of X after arranging in groups is torch.Size([4, 32, 8, 20, 20, 20, 20])\n",
      "Shapes of means: NoKeepDim: torch.Size([4, 32]) KeepDim: torch.Size([4, 32, 1, 1, 1, 1, 1])\n",
      "Shapes of vars: NoKeepDim: torch.Size([4, 32]) KeepDim: torch.Size([4, 32, 1, 1, 1, 1, 1])\n",
      "Intermediate shape just after normalization = torch.Size([4, 32, 8, 20, 20, 20, 20])\n",
      "Final shape after norm = torch.Size([4, 256, 20, 20, 20, 20])\n",
      "\n",
      "\n",
      "\n",
      "group_norm      | exact: False | approximate: True  | maxdiff: 2.86102294921875e-06\n",
      "conv2d          | exact: False | approximate: False | maxdiff: 2.384185791015625e-06\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Input tensor (batch of RGB images)\n",
    "B, C_in, H, W, Z, Y = 4, 256, 20, 20, 20, 20# Input: RGB images\n",
    "images = torch.randn(B, C_in, H, W, Z, Y)\n",
    "\n",
    "normModule = GroupNorm2D (32, 256, 4, affine=True)\n",
    "torch_group_norm = nn.GroupNorm(num_groups=32, num_channels=256, affine=True)\n",
    "\n",
    "answer = normModule (images)\n",
    "torch_answer = torch_group_norm (images)\n",
    "\n",
    "B, Cin, H, W = 1, 4, 400, 400\n",
    "out_channels, kernel_size = 3, (4, 4)\n",
    "stride, padding = (1, 1), (0, 0)\n",
    "\n",
    "torch.manual_seed(32)\n",
    "input = torch.randn(B, Cin, H, W)\n",
    "\n",
    "convLayer = Conv2D(Cin, out_channels, kernel_size, stride=stride, padding=padding, bias=True)\n",
    "myConv2D = convLayer(input)\n",
    "\n",
    "torchConv = nn.Conv2d(Cin, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
    "torchConv.weight = nn.Parameter(convLayer.weight.data.clone())\n",
    "torchConv.bias = nn.Parameter(convLayer.bias.data.clone())\n",
    "torchConv2D = torchConv(input)\n",
    "\n",
    "\n",
    "print (\"\\n\\n\")\n",
    "# TODO: Documentation; change tolerances later for approx,\n",
    "cmp (\"group_norm\", answer, torch_answer)\n",
    "cmp (\"conv2d\", myConv2D, torchConv2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tr2)",
   "language": "python",
   "name": "tr2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
