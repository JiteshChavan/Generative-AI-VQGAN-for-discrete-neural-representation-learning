{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zen\\miniconda3\\envs\\tr\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Zen\\miniconda3\\envs\\tr\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "e:\\Research\\vincentVanGogh\\lpips.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n"
     ]
    }
   ],
   "source": [
    "from vqgan import VQGan\n",
    "from vqgan import VQGan\n",
    "from encoder import EncoderConfig\n",
    "from decoder import DecoderConfig\n",
    "from quantizer import QuantizerConfig\n",
    "import torch.nn.functional as F\n",
    "from lpips import LPIPS\n",
    "from discriminator import Discriminator, DiscriminatorConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "disc = Discriminator (DiscriminatorConfig)\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "\n",
    "\n",
    "disc.to('cuda')\n",
    "percept_disc = LPIPS().eval()\n",
    "percept_disc.to('cuda')\n",
    "x = torch.randn (1, 3, 256,256).to('cuda')\n",
    "gan = VQGan (EncoderConfig, QuantizerConfig, DecoderConfig)\n",
    "gan.to('cuda')\n",
    "\n",
    "master_process = True\n",
    "steps_per_epoch = 500\n",
    "d_loss_factor = 1.0\n",
    "grad_accum_steps = 4\n",
    "lr = 6e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: DeprecationWarning: invalid escape sequence \\V\n",
      "<>:18: DeprecationWarning: invalid escape sequence \\R\n",
      "<>:12: DeprecationWarning: invalid escape sequence \\V\n",
      "<>:18: DeprecationWarning: invalid escape sequence \\R\n",
      "C:\\Users\\Zen\\AppData\\Local\\Temp\\ipykernel_29580\\1499343910.py:12: DeprecationWarning: invalid escape sequence \\V\n",
      "  print (\"\\VGG FORWARDED\\n\")\n",
      "C:\\Users\\Zen\\AppData\\Local\\Temp\\ipykernel_29580\\1499343910.py:18: DeprecationWarning: invalid escape sequence \\R\n",
      "  print (\"\\Rec loss computed\\n\")\n"
     ]
    }
   ],
   "source": [
    "def compute_loss (vqgan_model, discriminator, perceptual_distinguisher, x, perceptual_loss_factor, rec_loss_factor, current_step):\n",
    "    print (\"\\n inside compute loss \\n\")\n",
    "    reconstructed_iamges, encoding_indices, vq_loss = vqgan_model (x)\n",
    "    print (\"\\n vqgan model forwded\\n\")\n",
    "    # Discriminator Forward\n",
    "    disc_real = discriminator (x)\n",
    "    disc_generated = discriminator (reconstructed_iamges)\n",
    "    print (\"\\nDISC forwarded\\n\")\n",
    "    # VGG forward\n",
    "    perceptual_loss = perceptual_distinguisher (x, reconstructed_iamges) # (B, 1, 1, 1)\n",
    "    perceptual_loss = perceptual_loss.squeeze ( ) #(B)\n",
    "    print (\"\\VGG FORWARDED\\n\")\n",
    "    # formality none of the optimizers have access to vgg params but this way its explicit and efficient\n",
    "    perceptual_loss = perceptual_loss.detach ( )\n",
    "\n",
    "    reconstruction_loss = F.mse_loss (x, reconstructed_iamges, reduction='none') #(B, C, H, W)\n",
    "    reconstruction_loss = reconstruction_loss.mean (dim=(1,2,3)) #(B)\n",
    "    print (\"\\Rec loss computed\\n\")\n",
    "    perceptual_recon_loss = rec_loss_factor * reconstruction_loss + perceptual_loss_factor * perceptual_loss\n",
    "    perceptual_recon_loss = perceptual_recon_loss.mean()\n",
    "    print (\"\\ncombined perceptual rec loss computed\\n\")\n",
    "\n",
    "    # compute adversarial loss for generator if it's not zeroth epoch (VQGAN paper specs)\n",
    "    is_zeroth_epoch = True if current_step < steps_per_epoch else False\n",
    "    if is_zeroth_epoch:\n",
    "        # don't calculate\n",
    "        # Wasserstein loss, approximation of BCE (discriminator(generated), ones_like(generated))\n",
    "        # TODO: Internalize throughly\n",
    "        # good approximation of KLD between discriminator's predictions given generated inputs and ideal distribution of generated images being classified as real.\n",
    "        # basically mean negative log likelihood is approximation of F.cross_entropy (discriminator(generated), ones_like(generated)) which is approximation of \n",
    "        # KLD between (discriminator(generated), ones_like(generated)) which is proportional to\n",
    "        # BCE (discriminator(genrated), ones_like(generated))\n",
    "        g_loss = 0 # replace by formula\n",
    "        print (\"\\nlambda skipped\\n\")\n",
    "    else:\n",
    "        #calculate adversarial loss for generator\n",
    "        g_loss = - torch.mean (disc_generated)\n",
    "        lambda_factor = vqgan_model.compute_lambda (perceptual_recon_loss, g_loss)\n",
    "        g_loss = lambda_factor * g_loss\n",
    "        if master_process:\n",
    "            print (f\"\\nGanLoss{g_loss:.6f} activated!\\n\")\n",
    "        \n",
    "    # VQ GAN LOSS\n",
    "    # Net Generator Loss\n",
    "    vq_gan_loss = perceptual_recon_loss + vq_loss + g_loss\n",
    "    print (\"VQGAN LOSS COMPUTED\")\n",
    "                    \n",
    "    # Discriminator Loss : Hinge Loss. Push Logits for the catgeorical distribution that come out of discriminator \n",
    "    # to be more than +1 for real images\n",
    "    # At the same time Push logits for categorical distribution that comes out of discriminator to be less than -1 for fake images\n",
    "\n",
    "    # drive logits corresponding to real images from discriminator above zero\n",
    "    # don't need to explicitly squeeze out the spurious depth dimension (1 channels) from discriminator output, mean will still be evaluated correctly\n",
    "    d_loss_real= torch.mean(F.relu (1.0 - disc_real))\n",
    "    # drive logits corresponding to fake(generate) images from discriminator below -1\n",
    "    d_loss_fake = torch.mean(F.relu (1.0 + disc_generated))\n",
    "\n",
    "    d_loss = d_loss_factor * 0.5 * (d_loss_real + d_loss_fake)\n",
    "    return vq_gan_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "ddp = int (os.environ.get ('RANK', -1)) != -1 # is this a ddp run or not?\n",
    "if ddp:\n",
    "    assert torch.cuda.is_available(), f\"CUDA is required for DDP\"\n",
    "    init_process_group (backend='nccl')\n",
    "    ddp_rank = int (os.environ['RANK'])\n",
    "    ddp_local_rank = int (os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int (os.environ['WORLD_SIZE'])\n",
    "    device = f\"cuda:{ddp_local_rank}\"\n",
    "    torch.cuda.set_device (device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc\n",
    "else:\n",
    "    # non ddp vanilla run\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif hasattr (torch.backends, 'mps') and torch.backends.mps.is_available ():\n",
    "        device = 'mps'\n",
    "    print (f\"using device: {device}\")\n",
    "\n",
    "ddp = int (os.environ.get ('RANK', -1)) != -1 # is this a ddp run or not?\n",
    "if ddp:\n",
    "    assert torch.cuda.is_available(), f\"CUDA is required for DDP\"\n",
    "    init_process_group (backend='nccl')\n",
    "    ddp_rank = int (os.environ['RANK'])\n",
    "    ddp_local_rank = int (os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int (os.environ['WORLD_SIZE'])\n",
    "    device = f\"cuda:{ddp_local_rank}\"\n",
    "    torch.cuda.set_device (device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc\n",
    "else:\n",
    "    # non ddp vanilla run\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif hasattr (torch.backends, 'mps') and torch.backends.mps.is_available ():\n",
    "        device = 'mps'\n",
    "    print (f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ddp:\n",
    "    # forward is unchanged, backward is mostly unchanged except there is overlap between computation and communication of gradients\n",
    "    # while the backward pass is still going on, to average the gradients from all processes\n",
    "    # we're tacking on this average as we will see in a bit\n",
    "    gan = DDP (gan, device_ids=[ddp_local_rank])\n",
    "    disc = DDP (disc, device_ids=[ddp_local_rank])\n",
    "raw_gan = gan.module if ddp else gan\n",
    "raw_discriminator = disc.module if ddp else disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer_configuration for VQGAN model:\n",
      " num decayed parameter tensors:50 with 104367168 parameters\n",
      "num non-decayed parameter tensors:105 with 54467 parameters\n",
      "using fused AdamW:True\n"
     ]
    }
   ],
   "source": [
    "gan_opt = raw_gan.configure_optimizers (0.1, 6e-4, 'cuda')\n",
    "gan.train()\n",
    "gan_opt.zero_grad()\n",
    "vqgan_loss_accum = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " inside compute loss \n",
      "\n",
      "\n",
      "\tze computed\n",
      "\n",
      "\n",
      "\tapplying quantizer \n",
      "\n",
      "on\n",
      "torch.Size([1, 512, 16, 16])\n",
      "\n",
      "\t rearranged ze for computing L2 with codebook\n",
      "\n",
      "\n",
      "\t calculating distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\t calculated distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\tquantizer applied\n",
      "\n",
      "\n",
      " vqgan model forwded\n",
      "\n",
      "\n",
      "DISC forwarded\n",
      "\n",
      "\\VGG FORWARDED\n",
      "\n",
      "\\Rec loss computed\n",
      "\n",
      "\n",
      "combined perceptual rec loss computed\n",
      "\n",
      "\n",
      "GanLoss-1.209179 activated!\n",
      "\n",
      "VQGAN LOSS COMPUTED\n",
      "\n",
      " inside compute loss \n",
      "\n",
      "\n",
      "\tze computed\n",
      "\n",
      "\n",
      "\tapplying quantizer \n",
      "\n",
      "on\n",
      "torch.Size([1, 512, 16, 16])\n",
      "\n",
      "\t rearranged ze for computing L2 with codebook\n",
      "\n",
      "\n",
      "\t calculating distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\t calculated distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\tquantizer applied\n",
      "\n",
      "\n",
      " vqgan model forwded\n",
      "\n",
      "\n",
      "DISC forwarded\n",
      "\n",
      "\\VGG FORWARDED\n",
      "\n",
      "\\Rec loss computed\n",
      "\n",
      "\n",
      "combined perceptual rec loss computed\n",
      "\n",
      "\n",
      "GanLoss-1.209179 activated!\n",
      "\n",
      "VQGAN LOSS COMPUTED\n",
      "\n",
      " inside compute loss \n",
      "\n",
      "\n",
      "\tze computed\n",
      "\n",
      "\n",
      "\tapplying quantizer \n",
      "\n",
      "on\n",
      "torch.Size([1, 512, 16, 16])\n",
      "\n",
      "\t rearranged ze for computing L2 with codebook\n",
      "\n",
      "\n",
      "\t calculating distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\t calculated distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\tquantizer applied\n",
      "\n",
      "\n",
      " vqgan model forwded\n",
      "\n",
      "\n",
      "DISC forwarded\n",
      "\n",
      "\\VGG FORWARDED\n",
      "\n",
      "\\Rec loss computed\n",
      "\n",
      "\n",
      "combined perceptual rec loss computed\n",
      "\n",
      "\n",
      "GanLoss-1.209179 activated!\n",
      "\n",
      "VQGAN LOSS COMPUTED\n",
      "\n",
      " inside compute loss \n",
      "\n",
      "\n",
      "\tze computed\n",
      "\n",
      "\n",
      "\tapplying quantizer \n",
      "\n",
      "on\n",
      "torch.Size([1, 512, 16, 16])\n",
      "\n",
      "\t rearranged ze for computing L2 with codebook\n",
      "\n",
      "\n",
      "\t calculating distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\t calculated distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\tquantizer applied\n",
      "\n",
      "\n",
      " vqgan model forwded\n",
      "\n",
      "\n",
      "DISC forwarded\n",
      "\n",
      "\\VGG FORWARDED\n",
      "\n",
      "\\Rec loss computed\n",
      "\n",
      "\n",
      "combined perceptual rec loss computed\n",
      "\n",
      "\n",
      "GanLoss-1.209179 activated!\n",
      "\n",
      "VQGAN LOSS COMPUTED\n"
     ]
    }
   ],
   "source": [
    "for micro_step in range (grad_accum_steps):\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        vqgan_loss, dloss = compute_loss (gan, disc, percept_disc, x, 1.0, 1.0, 500)\n",
    "    vqgan_loss = vqgan_loss / grad_accum_steps\n",
    "    vqgan_loss_accum += vqgan_loss.detach()\n",
    "\n",
    "    is_last_micro_step = (micro_step == (grad_accum_steps - 1))\n",
    "    if ddp:\n",
    "        # very last backward will have the grad_sync flag as True\n",
    "        # for now this works, but not a good practice if pytorch takes the flag away\n",
    "        # averages gradients\n",
    "        gan.require_backward_grad_sync = is_last_micro_step\n",
    "        disc.require_backward_grad_sync = is_last_micro_step\n",
    "    \n",
    "    vqgan_loss.backward (retain_graph=True)\n",
    "    dloss.backward()\n",
    "\n",
    "if ddp:\n",
    "        # calculates average of loss_accum on all the ranks, and it deposits that average on all the ranks\n",
    "        # all the ranks will contain loss_accum averaged up\n",
    "        dist.all_reduce (vqgan_loss_accum, op= dist.ReduceOp.AVG)\n",
    "vqgan_norm = torch.nn.utils.clip_grad_norm_ (gan.parameters(), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in gan_opt.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.initial_conv.weight: grad strides = (27, 9, 3, 1), grad size = torch.Size([128, 3, 3, 3])\n",
      "encoder.initial_conv.bias: grad strides = (1,), grad size = torch.Size([128])\n",
      "encoder.model.1.conv_projection.weight: grad strides = (2304, 9, 3, 1), grad size = torch.Size([256, 256, 3, 3])\n",
      "encoder.model.1.conv_projection.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.1.block.0.group_norm.weight: grad strides = (1,), grad size = torch.Size([128])\n",
      "encoder.model.1.block.0.group_norm.bias: grad strides = (1,), grad size = torch.Size([128])\n",
      "encoder.model.1.block.2.weight: grad strides = (1152, 9, 3, 1), grad size = torch.Size([256, 128, 3, 3])\n",
      "encoder.model.1.block.2.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.1.block.3.group_norm.weight: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.1.block.3.group_norm.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.1.channel_up.weight: grad strides = (128, 1, 1, 1), grad size = torch.Size([256, 128, 1, 1])\n",
      "encoder.model.1.channel_up.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.2.conv.weight: grad strides = (2304, 9, 3, 1), grad size = torch.Size([256, 256, 3, 3])\n",
      "encoder.model.2.conv.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.3.conv_projection.weight: grad strides = (2304, 9, 3, 1), grad size = torch.Size([256, 256, 3, 3])\n",
      "encoder.model.3.conv_projection.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.3.block.0.group_norm.weight: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.3.block.0.group_norm.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.3.block.2.weight: grad strides = (2304, 9, 3, 1), grad size = torch.Size([256, 256, 3, 3])\n",
      "encoder.model.3.block.2.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.3.block.3.group_norm.weight: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.3.block.3.group_norm.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.4.conv.weight: grad strides = (2304, 9, 3, 1), grad size = torch.Size([256, 256, 3, 3])\n",
      "encoder.model.4.conv.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.5.conv_projection.weight: grad strides = (4608, 9, 3, 1), grad size = torch.Size([512, 512, 3, 3])\n",
      "encoder.model.5.conv_projection.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "encoder.model.5.block.0.group_norm.weight: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.5.block.0.group_norm.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "encoder.model.5.block.2.weight: grad strides = (2304, 9, 3, 1), grad size = torch.Size([512, 256, 3, 3])\n",
      "encoder.model.5.block.2.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "encoder.model.5.block.3.group_norm.weight: grad strides = (1,), grad size = torch.Size([512])\n",
      "encoder.model.5.block.3.group_norm.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "encoder.model.5.channel_up.weight: grad strides = (256, 1, 1, 1), grad size = torch.Size([512, 256, 1, 1])\n",
      "encoder.model.5.channel_up.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "encoder.model.6.conv.weight: grad strides = (4608, 9, 3, 1), grad size = torch.Size([512, 512, 3, 3])\n",
      "encoder.model.6.conv.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "encoder.model.7.conv_projection.weight: grad strides = (9216, 9, 3, 1), grad size = torch.Size([1024, 1024, 3, 3])\n",
      "encoder.model.7.conv_projection.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.7.block.0.group_norm.weight: grad strides = (1,), grad size = torch.Size([512])\n",
      "encoder.model.7.block.0.group_norm.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "encoder.model.7.block.2.weight: grad strides = (4608, 9, 3, 1), grad size = torch.Size([1024, 512, 3, 3])\n",
      "encoder.model.7.block.2.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.7.block.3.group_norm.weight: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.7.block.3.group_norm.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.7.channel_up.weight: grad strides = (512, 1, 1, 1), grad size = torch.Size([1024, 512, 1, 1])\n",
      "encoder.model.7.channel_up.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.8.conv.weight: grad strides = (9216, 9, 3, 1), grad size = torch.Size([1024, 1024, 3, 3])\n",
      "encoder.model.8.conv.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.9.conv_projection.weight: grad strides = (9216, 9, 3, 1), grad size = torch.Size([1024, 1024, 3, 3])\n",
      "encoder.model.9.conv_projection.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.9.block.0.group_norm.weight: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.9.block.0.group_norm.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.9.block.2.weight: grad strides = (9216, 9, 3, 1), grad size = torch.Size([1024, 1024, 3, 3])\n",
      "encoder.model.9.block.2.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.9.block.3.group_norm.weight: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.9.block.3.group_norm.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.10.group_norm.group_norm.weight: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.10.group_norm.group_norm.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.10.conv_attention.weight: grad strides = (1024, 1, 1, 1), grad size = torch.Size([3072, 1024, 1, 1])\n",
      "encoder.model.10.conv_attention.bias: grad strides = (1,), grad size = torch.Size([3072])\n",
      "encoder.model.10.conv_projection.weight: grad strides = (1024, 1, 1, 1), grad size = torch.Size([1024, 1024, 1, 1])\n",
      "encoder.model.10.conv_projection.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.11.conv_projection.weight: grad strides = (9216, 9, 3, 1), grad size = torch.Size([1024, 1024, 3, 3])\n",
      "encoder.model.11.conv_projection.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.11.block.0.group_norm.weight: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.11.block.0.group_norm.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.11.block.2.weight: grad strides = (9216, 9, 3, 1), grad size = torch.Size([1024, 1024, 3, 3])\n",
      "encoder.model.11.block.2.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.11.block.3.group_norm.weight: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.11.block.3.group_norm.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.12.group_norm.weight: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.12.group_norm.bias: grad strides = (1,), grad size = torch.Size([1024])\n",
      "encoder.model.14.weight: grad strides = (9216, 9, 3, 1), grad size = torch.Size([512, 1024, 3, 3])\n",
      "encoder.model.14.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "pre_quant_conv.weight: grad strides = (512, 1, 1, 1), grad size = torch.Size([512, 512, 1, 1])\n",
      "pre_quant_conv.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "quantizer.codebook.weight: grad strides = (512, 1), grad size = torch.Size([1024, 512])\n",
      "post_quant_conv.weight: grad strides = (512, 1, 1, 1), grad size = torch.Size([512, 512, 1, 1])\n",
      "post_quant_conv.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.0.weight: grad strides = (4608, 9, 3, 1), grad size = torch.Size([512, 512, 3, 3])\n",
      "decoder.model.0.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.1.conv_projection.weight: grad strides = (4608, 9, 3, 1), grad size = torch.Size([512, 512, 3, 3])\n",
      "decoder.model.1.conv_projection.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.1.block.0.group_norm.weight: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.1.block.0.group_norm.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.1.block.2.weight: grad strides = (4608, 9, 3, 1), grad size = torch.Size([512, 512, 3, 3])\n",
      "decoder.model.1.block.2.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.1.block.3.group_norm.weight: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.1.block.3.group_norm.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.2.group_norm.group_norm.weight: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.2.group_norm.group_norm.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.2.conv_attention.weight: grad strides = (512, 1, 1, 1), grad size = torch.Size([1536, 512, 1, 1])\n",
      "decoder.model.2.conv_attention.bias: grad strides = (1,), grad size = torch.Size([1536])\n",
      "decoder.model.2.conv_projection.weight: grad strides = (512, 1, 1, 1), grad size = torch.Size([512, 512, 1, 1])\n",
      "decoder.model.2.conv_projection.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.3.conv_projection.weight: grad strides = (4608, 9, 3, 1), grad size = torch.Size([512, 512, 3, 3])\n",
      "decoder.model.3.conv_projection.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.3.block.0.group_norm.weight: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.3.block.0.group_norm.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.3.block.2.weight: grad strides = (4608, 9, 3, 1), grad size = torch.Size([512, 512, 3, 3])\n",
      "decoder.model.3.block.2.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.3.block.3.group_norm.weight: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.3.block.3.group_norm.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.4.conv_projection.weight: grad strides = (4608, 9, 3, 1), grad size = torch.Size([512, 512, 3, 3])\n",
      "decoder.model.4.conv_projection.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.4.block.0.group_norm.weight: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.4.block.0.group_norm.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.4.block.2.weight: grad strides = (4608, 9, 3, 1), grad size = torch.Size([512, 512, 3, 3])\n",
      "decoder.model.4.block.2.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.4.block.3.group_norm.weight: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.4.block.3.group_norm.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.5.conv.weight: grad strides = (4608, 9, 3, 1), grad size = torch.Size([512, 512, 3, 3])\n",
      "decoder.model.5.conv.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.6.conv_projection.weight: grad strides = (2304, 9, 3, 1), grad size = torch.Size([256, 256, 3, 3])\n",
      "decoder.model.6.conv_projection.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "decoder.model.6.block.0.group_norm.weight: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.6.block.0.group_norm.bias: grad strides = (1,), grad size = torch.Size([512])\n",
      "decoder.model.6.block.2.weight: grad strides = (4608, 9, 3, 1), grad size = torch.Size([256, 512, 3, 3])\n",
      "decoder.model.6.block.2.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "decoder.model.6.block.3.group_norm.weight: grad strides = (1,), grad size = torch.Size([256])\n",
      "decoder.model.6.block.3.group_norm.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "decoder.model.6.channel_up.weight: grad strides = (512, 1, 1, 1), grad size = torch.Size([256, 512, 1, 1])\n",
      "decoder.model.6.channel_up.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "decoder.model.7.conv.weight: grad strides = (2304, 9, 3, 1), grad size = torch.Size([256, 256, 3, 3])\n",
      "decoder.model.7.conv.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "decoder.model.8.conv_projection.weight: grad strides = (1152, 9, 3, 1), grad size = torch.Size([128, 128, 3, 3])\n",
      "decoder.model.8.conv_projection.bias: grad strides = (1,), grad size = torch.Size([128])\n",
      "decoder.model.8.block.0.group_norm.weight: grad strides = (1,), grad size = torch.Size([256])\n",
      "decoder.model.8.block.0.group_norm.bias: grad strides = (1,), grad size = torch.Size([256])\n",
      "decoder.model.8.block.2.weight: grad strides = (2304, 9, 3, 1), grad size = torch.Size([128, 256, 3, 3])\n",
      "decoder.model.8.block.2.bias: grad strides = (1,), grad size = torch.Size([128])\n",
      "decoder.model.8.block.3.group_norm.weight: grad strides = (1,), grad size = torch.Size([128])\n",
      "decoder.model.8.block.3.group_norm.bias: grad strides = (1,), grad size = torch.Size([128])\n",
      "decoder.model.8.channel_up.weight: grad strides = (256, 1, 1, 1), grad size = torch.Size([128, 256, 1, 1])\n",
      "decoder.model.8.channel_up.bias: grad strides = (1,), grad size = torch.Size([128])\n",
      "decoder.model.9.conv.weight: grad strides = (1152, 9, 3, 1), grad size = torch.Size([128, 128, 3, 3])\n",
      "decoder.model.9.conv.bias: grad strides = (1,), grad size = torch.Size([128])\n",
      "decoder.model.10.conv_projection.weight: grad strides = (576, 9, 3, 1), grad size = torch.Size([64, 64, 3, 3])\n",
      "decoder.model.10.conv_projection.bias: grad strides = (1,), grad size = torch.Size([64])\n",
      "decoder.model.10.block.0.group_norm.weight: grad strides = (1,), grad size = torch.Size([128])\n",
      "decoder.model.10.block.0.group_norm.bias: grad strides = (1,), grad size = torch.Size([128])\n",
      "decoder.model.10.block.2.weight: grad strides = (1152, 9, 3, 1), grad size = torch.Size([64, 128, 3, 3])\n",
      "decoder.model.10.block.2.bias: grad strides = (1,), grad size = torch.Size([64])\n",
      "decoder.model.10.block.3.group_norm.weight: grad strides = (1,), grad size = torch.Size([64])\n",
      "decoder.model.10.block.3.group_norm.bias: grad strides = (1,), grad size = torch.Size([64])\n",
      "decoder.model.10.channel_up.weight: grad strides = (128, 1, 1, 1), grad size = torch.Size([64, 128, 1, 1])\n",
      "decoder.model.10.channel_up.bias: grad strides = (1,), grad size = torch.Size([64])\n",
      "decoder.model.11.conv.weight: grad strides = (576, 9, 3, 1), grad size = torch.Size([64, 64, 3, 3])\n",
      "decoder.model.11.conv.bias: grad strides = (1,), grad size = torch.Size([64])\n",
      "decoder.model.12.group_norm.weight: grad strides = (1,), grad size = torch.Size([64])\n",
      "decoder.model.12.group_norm.bias: grad strides = (1,), grad size = torch.Size([64])\n",
      "decoder.model.14.weight: grad strides = (576, 9, 3, 1), grad size = torch.Size([64, 64, 3, 3])\n",
      "decoder.model.14.bias: grad strides = (1,), grad size = torch.Size([64])\n",
      "decoder.model.15.weight: grad strides = (576, 9, 3, 1), grad size = torch.Size([3, 64, 3, 3])\n",
      "decoder.model.15.bias: grad strides = (1,), grad size = torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for name, param in gan.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: grad strides = {param.grad.stride()}, grad size = {param.grad.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VQGan' object has no attribute 'module'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m vqgan \u001b[38;5;241m=\u001b[39m VQGan (EncoderConfig, QuantizerConfig, DecoderConfig)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mvqgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Zen\\miniconda3\\envs\\tr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VQGan' object has no attribute 'module'"
     ]
    }
   ],
   "source": [
    "vqgan = VQGan (EncoderConfig, QuantizerConfig, DecoderConfig)\n",
    "print(vqgan.module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataloaderLite:\n",
    "    # to load B images for each ddp process with T= HW tokens each has 3 Channels (RGB)\n",
    "    def __init__(self, B, num_processes, process_rank, split, data_root):\n",
    "        self.B = B\n",
    "        self.num_processes = num_processes\n",
    "        self.process_rank = process_rank\n",
    "        assert split in {\"train\", \"val\"}, f\"Invalid split specified at DataLoaderInstatntiation\"\n",
    "        assert os.path.exists (data_root)\n",
    "        \n",
    "        # get the shard filenames\n",
    "        shards = os.listdir (data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        shards = sorted (shards)\n",
    "        self.shards = shards\n",
    "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
    "        if master_process:\n",
    "            print (f\"found {len(shards)} shards for split {split}\")\n",
    "        # state init at shard zero\n",
    "        self.reset ()\n",
    "    \n",
    "    def reset (self):\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens (self.shards[self.current_shard])\n",
    "        # B * T * process_rank\n",
    "        # don't really need T here\n",
    "        # B images go to one process\n",
    "        self.current_position = self.process_rank * self.B\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def next_batch (self):\n",
    "        B = self.B\n",
    "\n",
    "        # inputs\n",
    "        x = self.tokens [self.current_position : self.current_position + B] # (B, C, H, W) B might be less than B for last shard if total images are not perfectly divible into shards, syntax works\n",
    "        \n",
    "        # advance current position by one block (8*B in this case)\n",
    "        self.current_position += self.B * self.num_processes\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * self.num_processes) > len (self.tokens): # len (B, C, H, W) returns B\n",
    "            \n",
    "            if self.current_shard + 1 >= len(self.shards):\n",
    "                self.current_epoch = self.current_epoch + 1\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            \n",
    "            self.tokens = load_tokens (self.shards[self.current_shard])\n",
    "            self.current_position = self.process_rank * B\n",
    "            # total len (tokens) = 9B\n",
    "            # 0           1                                2                                3 4 5 6 7 \n",
    "            # 8B to 9B    9(x)[shard2 1B to 2B instead]    10(x)[shard2 2B to 3B instead]\n",
    "            # third gpu loads 2B-3B from next shard first two load remainder of the current shard\n",
    "            # if remainder is not divisible by in the last shard [a:b] loads from a to c (c < b)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 shards for split train\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_loader = DataloaderLite (4, 1, 0, \"train\", \"./dummy_tests/shards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = train_loader.tokens[:4,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_batch = train_loader.next_batch()\n",
    "loader_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference.allclose(loader_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shard_train_0001.npy', 'shard_val_0000.npy']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.listdir(\"./dummy_tests/shards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(train_loader.shards[train_loader.current_shard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3647, -0.3647, -0.3647,  ..., -0.8037, -0.8115, -0.8115],\n",
       "         [-0.3257, -0.3257, -0.3335,  ..., -0.7568, -0.7646, -0.7646],\n",
       "         [-0.2939, -0.2939, -0.3018,  ..., -0.6943, -0.7021, -0.7021],\n",
       "         ...,\n",
       "         [ 0.0820,  0.0430,  0.0195,  ..., -0.3491, -0.3413, -0.3335],\n",
       "         [ 0.0977,  0.0664,  0.0742,  ..., -0.4038, -0.3960, -0.3960],\n",
       "         [ 0.0273,  0.0117,  0.0352,  ..., -0.4351, -0.4272, -0.4194]],\n",
       "\n",
       "        [[-0.7021, -0.7021, -0.7021,  ..., -0.8589, -0.8667, -0.8667],\n",
       "         [-0.6631, -0.6631, -0.6709,  ..., -0.8271, -0.8354, -0.8354],\n",
       "         [-0.6313, -0.6313, -0.6392,  ..., -0.7803, -0.7881, -0.7881],\n",
       "         ...,\n",
       "         [-0.2705, -0.3096, -0.3335,  ..., -0.6470, -0.6392, -0.6313],\n",
       "         [-0.2393, -0.2705, -0.2627,  ..., -0.7100, -0.7021, -0.7021],\n",
       "         [-0.3096, -0.3257, -0.3018,  ..., -0.7412, -0.7334, -0.7256]],\n",
       "\n",
       "        [[-0.6470, -0.6470, -0.6470,  ..., -0.9058, -0.9136, -0.9136],\n",
       "         [-0.6079, -0.6079, -0.6157,  ..., -0.8823, -0.8901, -0.8901],\n",
       "         [-0.5762, -0.5762, -0.5840,  ..., -0.8271, -0.8354, -0.8354],\n",
       "         ...,\n",
       "         [-0.2314, -0.2705, -0.2939,  ..., -0.6548, -0.6470, -0.6392],\n",
       "         [-0.1841, -0.2158, -0.2080,  ..., -0.7021, -0.6943, -0.6943],\n",
       "         [-0.2549, -0.2705, -0.2471,  ..., -0.7334, -0.7256, -0.7178]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference[0,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens (filename):\n",
    "    npt = np.load (filename) # shape (B, H, W, C) because PIL was used to store\n",
    "    print(npt.dtype)\n",
    "    ptt = torch.from_numpy (npt).permute (0, 3, 1, 2)\n",
    "    print (ptt.dtype)\n",
    "    ptt = ptt.to(torch.float32) # to match conv layers data type\n",
    "    return ptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "a = load_tokens(\"./dummy_tests/shards/shard_train_0001.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
