{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zen\\miniconda3\\envs\\tr\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Zen\\miniconda3\\envs\\tr\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "e:\\Research\\vincentVanGogh\\resnet_experiment\\lpips.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the root folder to Python's module search path\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from resnet_vqgan import VQGan\n",
    "from resnet_encoder import ResNetEncoderConfig\n",
    "from resnet_decoder import ResNetDecoderConfig\n",
    "from quantizer import QuantizerConfig\n",
    "import torch.nn.functional as F\n",
    "from lpips import LPIPS\n",
    "from discriminator import Discriminator, DiscriminatorConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "disc = Discriminator (DiscriminatorConfig)\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "\n",
    "\n",
    "disc.to('cuda')\n",
    "percept_disc = LPIPS().eval()\n",
    "percept_disc.to('cuda')\n",
    "x = torch.randn (1, 3, 256,256).to('cuda')\n",
    "gan = VQGan ()\n",
    "gan.to('cuda')\n",
    "\n",
    "master_process = True\n",
    "steps_per_epoch = 500\n",
    "d_loss_factor = 1.0\n",
    "grad_accum_steps = 4\n",
    "lr = 6e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: DeprecationWarning: invalid escape sequence \\V\n",
      "<>:18: DeprecationWarning: invalid escape sequence \\R\n",
      "<>:12: DeprecationWarning: invalid escape sequence \\V\n",
      "<>:18: DeprecationWarning: invalid escape sequence \\R\n",
      "C:\\Users\\Zen\\AppData\\Local\\Temp\\ipykernel_29580\\1499343910.py:12: DeprecationWarning: invalid escape sequence \\V\n",
      "  print (\"\\VGG FORWARDED\\n\")\n",
      "C:\\Users\\Zen\\AppData\\Local\\Temp\\ipykernel_29580\\1499343910.py:18: DeprecationWarning: invalid escape sequence \\R\n",
      "  print (\"\\Rec loss computed\\n\")\n"
     ]
    }
   ],
   "source": [
    "def compute_loss (vqgan_model, discriminator, perceptual_distinguisher, x, perceptual_loss_factor, rec_loss_factor, current_step):\n",
    "    print (\"\\n inside compute loss \\n\")\n",
    "    reconstructed_iamges, encoding_indices, vq_loss = vqgan_model (x)\n",
    "    print (\"\\n vqgan model forwded\\n\")\n",
    "    # Discriminator Forward\n",
    "    disc_real = discriminator (x)\n",
    "    disc_generated = discriminator (reconstructed_iamges)\n",
    "    print (\"\\nDISC forwarded\\n\")\n",
    "    # VGG forward\n",
    "    perceptual_loss = perceptual_distinguisher (x, reconstructed_iamges) # (B, 1, 1, 1)\n",
    "    perceptual_loss = perceptual_loss.squeeze ( ) #(B)\n",
    "    print (\"\\VGG FORWARDED\\n\")\n",
    "    # formality none of the optimizers have access to vgg params but this way its explicit and efficient\n",
    "    perceptual_loss = perceptual_loss.detach ( )\n",
    "\n",
    "    reconstruction_loss = F.mse_loss (x, reconstructed_iamges, reduction='none') #(B, C, H, W)\n",
    "    reconstruction_loss = reconstruction_loss.mean (dim=(1,2,3)) #(B)\n",
    "    print (\"\\Rec loss computed\\n\")\n",
    "    perceptual_recon_loss = rec_loss_factor * reconstruction_loss + perceptual_loss_factor * perceptual_loss\n",
    "    perceptual_recon_loss = perceptual_recon_loss.mean()\n",
    "    print (\"\\ncombined perceptual rec loss computed\\n\")\n",
    "\n",
    "    # compute adversarial loss for generator if it's not zeroth epoch (VQGAN paper specs)\n",
    "    is_zeroth_epoch = True if current_step < steps_per_epoch else False\n",
    "    if is_zeroth_epoch:\n",
    "        # don't calculate\n",
    "        # Wasserstein loss, approximation of BCE (discriminator(generated), ones_like(generated))\n",
    "        # TODO: Internalize throughly\n",
    "        # good approximation of KLD between discriminator's predictions given generated inputs and ideal distribution of generated images being classified as real.\n",
    "        # basically mean negative log likelihood is approximation of F.cross_entropy (discriminator(generated), ones_like(generated)) which is approximation of \n",
    "        # KLD between (discriminator(generated), ones_like(generated)) which is proportional to\n",
    "        # BCE (discriminator(genrated), ones_like(generated))\n",
    "        g_loss = 0 # replace by formula\n",
    "        print (\"\\nlambda skipped\\n\")\n",
    "    else:\n",
    "        #calculate adversarial loss for generator\n",
    "        g_loss = - torch.mean (disc_generated)\n",
    "        lambda_factor = vqgan_model.compute_lambda (perceptual_recon_loss, g_loss)\n",
    "        g_loss = lambda_factor * g_loss\n",
    "        if master_process:\n",
    "            print (f\"\\nGanLoss{g_loss:.6f} activated!\\n\")\n",
    "        \n",
    "    # VQ GAN LOSS\n",
    "    # Net Generator Loss\n",
    "    vq_gan_loss = perceptual_recon_loss + vq_loss + g_loss\n",
    "    print (\"VQGAN LOSS COMPUTED\")\n",
    "                    \n",
    "    # Discriminator Loss : Hinge Loss. Push Logits for the catgeorical distribution that come out of discriminator \n",
    "    # to be more than +1 for real images\n",
    "    # At the same time Push logits for categorical distribution that comes out of discriminator to be less than -1 for fake images\n",
    "\n",
    "    # drive logits corresponding to real images from discriminator above zero\n",
    "    # don't need to explicitly squeeze out the spurious depth dimension (1 channels) from discriminator output, mean will still be evaluated correctly\n",
    "    d_loss_real= torch.mean(F.relu (1.0 - disc_real))\n",
    "    # drive logits corresponding to fake(generate) images from discriminator below -1\n",
    "    d_loss_fake = torch.mean(F.relu (1.0 + disc_generated))\n",
    "\n",
    "    d_loss = d_loss_factor * 0.5 * (d_loss_real + d_loss_fake)\n",
    "    return vq_gan_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "ddp = int (os.environ.get ('RANK', -1)) != -1 # is this a ddp run or not?\n",
    "if ddp:\n",
    "    assert torch.cuda.is_available(), f\"CUDA is required for DDP\"\n",
    "    init_process_group (backend='nccl')\n",
    "    ddp_rank = int (os.environ['RANK'])\n",
    "    ddp_local_rank = int (os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int (os.environ['WORLD_SIZE'])\n",
    "    device = f\"cuda:{ddp_local_rank}\"\n",
    "    torch.cuda.set_device (device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc\n",
    "else:\n",
    "    # non ddp vanilla run\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif hasattr (torch.backends, 'mps') and torch.backends.mps.is_available ():\n",
    "        device = 'mps'\n",
    "    print (f\"using device: {device}\")\n",
    "\n",
    "ddp = int (os.environ.get ('RANK', -1)) != -1 # is this a ddp run or not?\n",
    "if ddp:\n",
    "    assert torch.cuda.is_available(), f\"CUDA is required for DDP\"\n",
    "    init_process_group (backend='nccl')\n",
    "    ddp_rank = int (os.environ['RANK'])\n",
    "    ddp_local_rank = int (os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int (os.environ['WORLD_SIZE'])\n",
    "    device = f\"cuda:{ddp_local_rank}\"\n",
    "    torch.cuda.set_device (device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc\n",
    "else:\n",
    "    # non ddp vanilla run\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif hasattr (torch.backends, 'mps') and torch.backends.mps.is_available ():\n",
    "        device = 'mps'\n",
    "    print (f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ddp:\n",
    "    # forward is unchanged, backward is mostly unchanged except there is overlap between computation and communication of gradients\n",
    "    # while the backward pass is still going on, to average the gradients from all processes\n",
    "    # we're tacking on this average as we will see in a bit\n",
    "    gan = DDP (gan, device_ids=[ddp_local_rank])\n",
    "    disc = DDP (disc, device_ids=[ddp_local_rank])\n",
    "raw_gan = gan.module if ddp else gan\n",
    "raw_discriminator = disc.module if ddp else disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer_configuration for VQGAN model:\n",
      " num decayed parameter tensors:50 with 104367168 parameters\n",
      "num non-decayed parameter tensors:105 with 54467 parameters\n",
      "using fused AdamW:True\n"
     ]
    }
   ],
   "source": [
    "gan_opt = raw_gan.configure_optimizers (0.1, 6e-4, 'cuda')\n",
    "gan.train()\n",
    "gan_opt.zero_grad()\n",
    "vqgan_loss_accum = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " inside compute loss \n",
      "\n",
      "\n",
      "\tze computed\n",
      "\n",
      "\n",
      "\tapplying quantizer \n",
      "\n",
      "on\n",
      "torch.Size([1, 512, 16, 16])\n",
      "\n",
      "\t rearranged ze for computing L2 with codebook\n",
      "\n",
      "\n",
      "\t calculating distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\t calculated distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\tquantizer applied\n",
      "\n",
      "\n",
      " vqgan model forwded\n",
      "\n",
      "\n",
      "DISC forwarded\n",
      "\n",
      "\\VGG FORWARDED\n",
      "\n",
      "\\Rec loss computed\n",
      "\n",
      "\n",
      "combined perceptual rec loss computed\n",
      "\n",
      "\n",
      "GanLoss-1.209179 activated!\n",
      "\n",
      "VQGAN LOSS COMPUTED\n",
      "\n",
      " inside compute loss \n",
      "\n",
      "\n",
      "\tze computed\n",
      "\n",
      "\n",
      "\tapplying quantizer \n",
      "\n",
      "on\n",
      "torch.Size([1, 512, 16, 16])\n",
      "\n",
      "\t rearranged ze for computing L2 with codebook\n",
      "\n",
      "\n",
      "\t calculating distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\t calculated distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\tquantizer applied\n",
      "\n",
      "\n",
      " vqgan model forwded\n",
      "\n",
      "\n",
      "DISC forwarded\n",
      "\n",
      "\\VGG FORWARDED\n",
      "\n",
      "\\Rec loss computed\n",
      "\n",
      "\n",
      "combined perceptual rec loss computed\n",
      "\n",
      "\n",
      "GanLoss-1.209179 activated!\n",
      "\n",
      "VQGAN LOSS COMPUTED\n",
      "\n",
      " inside compute loss \n",
      "\n",
      "\n",
      "\tze computed\n",
      "\n",
      "\n",
      "\tapplying quantizer \n",
      "\n",
      "on\n",
      "torch.Size([1, 512, 16, 16])\n",
      "\n",
      "\t rearranged ze for computing L2 with codebook\n",
      "\n",
      "\n",
      "\t calculating distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\t calculated distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\tquantizer applied\n",
      "\n",
      "\n",
      " vqgan model forwded\n",
      "\n",
      "\n",
      "DISC forwarded\n",
      "\n",
      "\\VGG FORWARDED\n",
      "\n",
      "\\Rec loss computed\n",
      "\n",
      "\n",
      "combined perceptual rec loss computed\n",
      "\n",
      "\n",
      "GanLoss-1.209179 activated!\n",
      "\n",
      "VQGAN LOSS COMPUTED\n",
      "\n",
      " inside compute loss \n",
      "\n",
      "\n",
      "\tze computed\n",
      "\n",
      "\n",
      "\tapplying quantizer \n",
      "\n",
      "on\n",
      "torch.Size([1, 512, 16, 16])\n",
      "\n",
      "\t rearranged ze for computing L2 with codebook\n",
      "\n",
      "\n",
      "\t calculating distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\t calculated distances for each of torch.Size([256, 1, 512]) with codebook vectors torch.Size([1024, 512])\n",
      "\n",
      "\n",
      "\tquantizer applied\n",
      "\n",
      "\n",
      " vqgan model forwded\n",
      "\n",
      "\n",
      "DISC forwarded\n",
      "\n",
      "\\VGG FORWARDED\n",
      "\n",
      "\\Rec loss computed\n",
      "\n",
      "\n",
      "combined perceptual rec loss computed\n",
      "\n",
      "\n",
      "GanLoss-1.209179 activated!\n",
      "\n",
      "VQGAN LOSS COMPUTED\n"
     ]
    }
   ],
   "source": [
    "for micro_step in range (grad_accum_steps):\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        vqgan_loss, dloss = compute_loss (gan, disc, percept_disc, x, 1.0, 1.0, 500)\n",
    "    vqgan_loss = vqgan_loss / grad_accum_steps\n",
    "    vqgan_loss_accum += vqgan_loss.detach()\n",
    "\n",
    "    is_last_micro_step = (micro_step == (grad_accum_steps - 1))\n",
    "    if ddp:\n",
    "        # very last backward will have the grad_sync flag as True\n",
    "        # for now this works, but not a good practice if pytorch takes the flag away\n",
    "        # averages gradients\n",
    "        gan.require_backward_grad_sync = is_last_micro_step\n",
    "        disc.require_backward_grad_sync = is_last_micro_step\n",
    "    \n",
    "    vqgan_loss.backward (retain_graph=True)\n",
    "    dloss.backward()\n",
    "\n",
    "if ddp:\n",
    "        # calculates average of loss_accum on all the ranks, and it deposits that average on all the ranks\n",
    "        # all the ranks will contain loss_accum averaged up\n",
    "        dist.all_reduce (vqgan_loss_accum, op= dist.ReduceOp.AVG)\n",
    "vqgan_norm = torch.nn.utils.clip_grad_norm_ (gan.parameters(), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in gan_opt.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_opt.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
