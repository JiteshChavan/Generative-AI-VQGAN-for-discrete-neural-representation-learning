{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull out specified number of images from a folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 1600 images to ./crushit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_images(source_dir, destination_dir, num_images=1600):\n",
    "    \"\"\"\n",
    "    Copies the first specified number of images from a source folder to a destination folder.\n",
    "    \n",
    "    Parameters:\n",
    "        source_dir (str): Path to the folder containing the original images.\n",
    "        destination_dir (str): Path to the folder where selected images will be copied.\n",
    "        num_images (int): Number of images to copy.\n",
    "    \"\"\"\n",
    "    # Ensure source directory exists\n",
    "    if not os.path.exists(source_dir):\n",
    "        raise FileNotFoundError(f\"Source directory not found: {source_dir}\")\n",
    "    \n",
    "    # Ensure destination directory exists; create it if it doesn't\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "    \n",
    "    # List all files in the source directory (alphabetically sorted)\n",
    "    all_files = ([f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))])\n",
    "    \n",
    "    # Ensure there are enough files in the source directory\n",
    "    if len(all_files) < num_images:\n",
    "        raise ValueError(f\"Source directory contains only {len(all_files)} files, but {num_images} are requested.\")\n",
    "    \n",
    "    # Select the first `num_images` files\n",
    "    selected_files = all_files[:num_images]\n",
    "    \n",
    "    # Copy the selected files to the destination directory\n",
    "    for file_name in selected_files:\n",
    "        src_path = os.path.join(source_dir, file_name)\n",
    "        dest_path = os.path.join(destination_dir, file_name)\n",
    "        shutil.copy(src_path, dest_path)\n",
    "    \n",
    "    print(f\"Copied {num_images} images to {destination_dir}\")\n",
    "\n",
    "# Example usage\n",
    "source_folder = \"./oxfordflowers/jpg\"  # Replace with your source folder path\n",
    "destination_folder = \"./crushit\"  # Replace with your destination folder path\n",
    "number_of_images = 1600\n",
    "\n",
    "copy_images(source_folder, destination_folder, number_of_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale down smaller size to 256 maintaning aspect ratio then return center crop of 256x256 in specified output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Preprocess an image to maintain the original aspect ratio and apply a 256x256 center crop.\n",
    "    \n",
    "    Parameters:\n",
    "        image_path (str): Path to the input image.\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image.Image: Preprocessed image.\n",
    "    \"\"\"\n",
    "    # Define the transformation to resize the smaller dimension and center crop to 256x256\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),  # Resize the smaller dimension to 256 (maintains aspect ratio)\n",
    "        transforms.CenterCrop(256)  # Center crop to 256x256\n",
    "    ])\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Convert to RGB if the image is not already in RGB mode\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    \n",
    "    # Apply the transformations\n",
    "    processed_image = transform(image)\n",
    "    \n",
    "    return processed_image\n",
    "\n",
    "def process_images_in_folder(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process all images in a specified folder and save them after applying the preprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "        input_folder (str): Folder containing the input images.\n",
    "        output_folder (str): Folder where the processed images will be saved.\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Iterate through the images in the input folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        # Check for image files (can add more file types if needed)\n",
    "        if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(input_folder, file_name)\n",
    "            \n",
    "            # Preprocess the image\n",
    "            processed_image = preprocess_image(image_path)\n",
    "            \n",
    "            # Save the processed image to the output folder\n",
    "            output_path = os.path.join(output_folder, file_name)\n",
    "            processed_image.save(output_path)\n",
    "            #print(f\"Processed and saved: {output_path}\")\n",
    "\n",
    "# Set the input folder containing images and the output folder to save processed images\n",
    "input_folder = './crushit'  # Change this to your folder path with images\n",
    "output_folder = './crushit2'  # Change this to where you want to save the processed images\n",
    "\n",
    "# Process the images in the folder\n",
    "process_images_in_folder(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pack images into shards of npy format, swith specified batch size for each shard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n",
      "torch.Size([64, 3, 256, 256])\n",
      "cuda:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Example of iterating through the DataLoader and printing the shape of batches\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should print (B, C, H, W) where B = batch size, C = 3 (RGB), H = height, W = width\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Check if the images are on the GPU (cuda) or CPU\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Zen\\miniconda3\\envs\\tr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Zen\\miniconda3\\envs\\tr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Zen\\miniconda3\\envs\\tr\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Zen\\miniconda3\\envs\\tr\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[28], line 31\u001b[0m, in \u001b[0;36mCustomImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     30\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]\n\u001b[1;32m---> 31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Open the image and ensure it's RGB\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     34\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)  \u001b[38;5;66;03m# Apply the transformation (to tensor and normalization)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Zen\\miniconda3\\envs\\tr\\lib\\site-packages\\PIL\\Image.py:3469\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3466\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3469\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3470\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Set the device to CUDA if available, otherwise fallback to CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Transformation: convert images to tensor and normalize to [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts the image to a Tensor (C, H, W) and normalizes to [0, 1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet stats\n",
    "    # This step converts the normalized values from [0, 1] -> [-1, 1]\n",
    "    transforms.Lambda(lambda x: 2 * x - 1)  # This scales the normalized tensor to [-1, 1]\n",
    "])\n",
    "\n",
    "# Define a custom Dataset to load images from a flat folder (no subfolders)\n",
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        # List all files in the folder\n",
    "        self.image_paths = [os.path.join(image_folder, fname) for fname in os.listdir(image_folder) if fname.endswith(('jpg', 'png', 'jpeg'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')  # Open the image and ensure it's RGB\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply the transformation (to tensor and normalization)\n",
    "        \n",
    "        # Move the image tensor to the GPU if available\n",
    "        image = image.to(device)  # Transfer the image tensor to the selected device (GPU or CPU)\n",
    "        \n",
    "        return image\n",
    "\n",
    "# Load your images as a torch dataset\n",
    "image_folder = \"./crushit2\"  # Replace with the actual path to your image folder\n",
    "dataset = CustomImageDataset(image_folder, transform=transform)\n",
    "\n",
    "# Use DataLoader to load images in batches (B, C, H, W)\n",
    "batch_size = 64  # Set the batch size\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Example of iterating through the DataLoader and printing the shape of batches\n",
    "for images in dataloader:\n",
    "    print(images.shape)  # Should print (B, C, H, W) where B = batch size, C = 3 (RGB), H = height, W = width\n",
    "    print(images.device)  # Check if the images are on the GPU (cuda) or CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
